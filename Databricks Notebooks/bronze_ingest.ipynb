{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed6edad9-971e-4ce0-a0ed-6c52ebe9ba20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook: bronze_ingest\n",
    "# Notebook path: /Workspace/Users/you/bronze_ingest\n",
    "# Expects widget/base parameter: ingestion_batch_id (string). If not provided, it will pick the oldest Pending batch.\n",
    "\n",
    "from datetime import datetime\n",
    "import json\n",
    "import hashlib\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType, DoubleType\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "FILE_REG_TABLE = \"census.bronze.file_registry_v1\"\n",
    "BRONZE_TABLE = \"census.bronze.individuals_raw_v1\"\n",
    "INGESTION_AUDIT = \"census.bronze.ingestion_audit_v1\"\n",
    "VALIDATION_REPORTS = \"census.bronze.validation_reports_v1\"\n",
    "MANIFEST_TOLERANCE_PCT = 0.001  # light check; main validation notebook enforces authoritative tolerance\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def now():\n",
    "    return datetime.utcnow()\n",
    "\n",
    "def tidy_status_col(col):\n",
    "    return F.lower(F.trim(F.coalesce(F.col(col), F.lit(\"\"))))\n",
    "\n",
    "def list_pending_batches():\n",
    "    fr = spark.table(FILE_REG_TABLE)\n",
    "    s = fr.withColumn(\"status_norm\", tidy_status_col(\"ingestion_status\"))\n",
    "    agg = s.groupBy(\"ingestion_batch_id\").agg(\n",
    "        F.count(F.when(F.col(\"status_norm\") == \"pending\", True)).alias(\"pending_count\"),\n",
    "        F.count(F.when(F.col(\"status_norm\") == \"processing\", True)).alias(\"processing_count\"),\n",
    "        F.count(F.when(F.col(\"status_norm\") == \"succeeded\", True)).alias(\"succeeded_count\"),\n",
    "        F.count(F.when(F.col(\"status_norm\") == \"failed\", True)).alias(\"failed_count\"),\n",
    "        F.count(F.lit(1)).alias(\"total_files\")\n",
    "    )\n",
    "    return [r.asDict() for r in agg.orderBy(F.desc(\"pending_count\"), F.asc(\"ingestion_batch_id\")).collect()]\n",
    "\n",
    "# ---------- Determine ingestion_batch_id and pending rows ----------\n",
    "try:\n",
    "    ingestion_batch_id_widget = dbutils.widgets.get(\"ingestion_batch_id\")\n",
    "    if ingestion_batch_id_widget is not None and ingestion_batch_id_widget.strip() == \"\":\n",
    "        ingestion_batch_id_widget = None\n",
    "except Exception:\n",
    "    ingestion_batch_id_widget = None\n",
    "\n",
    "file_registry_df = spark.table(FILE_REG_TABLE).withColumn(\"status_norm\", tidy_status_col(\"ingestion_status\"))\n",
    "batches_summary = list_pending_batches()\n",
    "\n",
    "if ingestion_batch_id_widget:\n",
    "    pending_df = file_registry_df.filter((F.col(\"ingestion_batch_id\") == ingestion_batch_id_widget) & (F.col(\"status_norm\") == \"pending\"))\n",
    "    pending = pending_df.collect()\n",
    "    if not pending:\n",
    "        msg = {\n",
    "            \"status\": \"NO_PENDING_FOR_BATCH\",\n",
    "            \"requested_batch\": ingestion_batch_id_widget,\n",
    "            \"available_batches_summary\": batches_summary,\n",
    "            \"message\": f\"No registry rows with ingestion_status='Pending' for ingestion_batch_id={ingestion_batch_id_widget}\"\n",
    "        }\n",
    "        print(json.dumps(msg, indent=2))\n",
    "        dbutils.notebook.exit(json.dumps(msg))\n",
    "    chosen_batch = ingestion_batch_id_widget\n",
    "else:\n",
    "    pending_batches = file_registry_df.filter(F.col(\"status_norm\") == \"pending\").select(\"ingestion_batch_id\",\"created_at\").distinct()\n",
    "    if pending_batches.limit(1).count() == 0:\n",
    "        msg = {\n",
    "            \"status\":\"NO_PENDING\",\n",
    "            \"message\":\"No pending files found for ingestion (no ingestion_batch_id was supplied and no registry rows with ingestion_status='Pending').\",\n",
    "            \"available_batches_summary\": batches_summary\n",
    "        }\n",
    "        print(json.dumps(msg, indent=2))\n",
    "        dbutils.notebook.exit(json.dumps(msg))\n",
    "    chosen_row = pending_batches.orderBy(F.asc(\"created_at\")).first()\n",
    "    chosen_batch = chosen_row[\"ingestion_batch_id\"]\n",
    "    print(f\"Auto-selected ingestion_batch_id = {chosen_batch} (oldest batch with Pending files)\")\n",
    "\n",
    "pending_df = file_registry_df.filter((F.col(\"ingestion_batch_id\") == chosen_batch) & (F.col(\"status_norm\") == \"pending\"))\n",
    "pending = pending_df.collect()\n",
    "\n",
    "if not pending:\n",
    "    msg = {\n",
    "        \"status\":\"NO_PENDING_POST_SELECT\",\n",
    "        \"chosen_batch\": chosen_batch,\n",
    "        \"available_batches_summary\": batches_summary,\n",
    "        \"message\": \"After selecting batch, no pending rows found. This indicates a race or state change; please re-run registration or check file_registry.\"\n",
    "    }\n",
    "    print(json.dumps(msg, indent=2))\n",
    "    dbutils.notebook.exit(json.dumps(msg))\n",
    "\n",
    "print(\"Selected batch:\", chosen_batch)\n",
    "print(\"Pending files count:\", len(pending))\n",
    "print(\"Pending files sample:\", [r[\"filename\"] for r in pending[:10]])\n",
    "\n",
    "ingestion_batch_id = chosen_batch\n",
    "run_id = f\"bronze-{ingestion_batch_id}-{now().strftime('%Y%m%dT%H%M%SZ')}\"\n",
    "start_ts = now()\n",
    "\n",
    "file_registry = DeltaTable.forName(spark, FILE_REG_TABLE)\n",
    "\n",
    "for r in pending:\n",
    "    fname = r[\"filename\"]\n",
    "    file_registry.update(\n",
    "        condition = F.expr(f\"filename = '{fname}'\"),\n",
    "        set = {\n",
    "            \"ingestion_status\": F.lit(\"Processing\"),\n",
    "            \"ingestion_attempts\": F.coalesce(F.col(\"ingestion_attempts\"), F.lit(0)) + F.lit(1),\n",
    "            \"last_ingestion_timestamp\": F.lit(datetime.utcnow()),\n",
    "            \"updated_at\": F.lit(datetime.utcnow())\n",
    "        }\n",
    "    )\n",
    "\n",
    "# ---------- Read files and coalesce schema (robust) ----------\n",
    "conformed_list = []\n",
    "errors = []\n",
    "processed_files = []\n",
    "\n",
    "# canonical mapping\n",
    "canonical_cols = {\n",
    "    \"person_id\": [\"person_id\",\"personId\",\"id\"],\n",
    "    \"household_id\": [\"household_id\",\"householdId\",\"hh_id\"],\n",
    "    \"geoid\": [\"geoid\",\"geo_id\",\"region_id\"],\n",
    "    \"region_code_legacy\": [\"region_code_legacy\",\"region_code\"],\n",
    "    \"region_name_reported\": [\"region_name_reported\",\"region_name\",\"regionName\"],\n",
    "    \"census_year\": [\"census_year\",\"year\"],\n",
    "    \"date_of_birth\": [\"date_of_birth\",\"dob\",\"DOB\",\"birth_date\"],\n",
    "    \"age\": [\"age\",\"Age\"],\n",
    "    \"sex\": [\"sex\",\"gender\"],\n",
    "    \"ethnicity_code\": [\"ethnicity_code\",\"ethnicity\"],\n",
    "    \"education_level\": [\"education_level\",\"education\"],\n",
    "    \"literacy\": [\"literacy\",\"is_literate\"],\n",
    "    \"employment_status\": [\"employment_status\",\"employed_status\"],\n",
    "    \"employment_type\": [\"employment_type\",\"employer_type\"],\n",
    "    \"industry_code\": [\"industry_code\",\"industry\"],\n",
    "    \"annual_income_local\": [\"annual_income_local\",\"income\",\"income_local\"],\n",
    "    \"marital_status\": [\"marital_status\",\"marital\"],\n",
    "    \"migration_status\": [\"migration_status\"],\n",
    "    \"arrival_year\": [\"arrival_year\"],\n",
    "    \"is_head_of_household\": [\"is_head_of_household\",\"head_of_household\"],\n",
    "    \"record_confidence_score\": [\"record_confidence_score\",\"confidence_score\"],\n",
    "    \"enumeration_source\": [\"enumeration_source\"],\n",
    "    \"national_id\": [\"national_id\",\"nationalId\"],\n",
    "    \"last_updated\": [\"last_updated\",\"updated_at\"]\n",
    "}\n",
    "\n",
    "all_variant_names = set()\n",
    "for variants in canonical_cols.values():\n",
    "    all_variant_names.update(variants)\n",
    "\n",
    "for row in pending:\n",
    "    fname = row[\"filename\"]\n",
    "    fpath = row[\"filepath\"]\n",
    "    try:\n",
    "        # read file robustly\n",
    "        try:\n",
    "            src = spark.read.format(\"parquet\").load(fpath)\n",
    "        except Exception as e_par:\n",
    "            src = spark.read.option(\"header\",\"true\").option(\"sep\",\";\").option(\"encoding\",\"latin1\").option(\"inferSchema\",\"false\").csv(fpath)\n",
    "        src_cols = src.columns\n",
    "\n",
    "        # compute extras = original src columns not consumed by canonical variants\n",
    "        extras = [c for c in src_cols if c not in all_variant_names]\n",
    "\n",
    "        # build exprs for canonical projection (use first matching variant if present)\n",
    "        exprs = []\n",
    "        for canon, variants in canonical_cols.items():\n",
    "            existing = [v for v in variants if v in src_cols]\n",
    "            if existing:\n",
    "                # cast where appropriate\n",
    "                if canon == \"annual_income_local\":\n",
    "                    exprs.append(F.col(existing[0]).cast(DoubleType()).alias(canon))\n",
    "                elif canon in (\"age\",\"geoid\",\"arrival_year\"):\n",
    "                    exprs.append(F.col(existing[0]).cast(\"int\").alias(canon))\n",
    "                elif canon == \"record_confidence_score\":\n",
    "                    exprs.append(F.col(existing[0]).cast(\"double\").alias(canon))\n",
    "                elif canon == \"last_updated\":\n",
    "                    exprs.append(F.to_timestamp(F.col(existing[0])).alias(canon))\n",
    "                else:\n",
    "                    exprs.append(F.col(existing[0]).alias(canon))\n",
    "            else:\n",
    "                exprs.append(F.lit(None).alias(canon))\n",
    "\n",
    "        # select canonical columns AND extras \n",
    "        # Note: selecting extras here keeps those original columns available to construct _raw_payload_json\n",
    "        select_cols = exprs + [F.col(c) for c in extras]\n",
    "        selected = src.select(*select_cols)\n",
    "\n",
    "        # normalize sex using the canonical alias 'sex' \n",
    "        selected = selected.withColumn(\"sex\",\n",
    "            F.when(F.lower(F.trim(F.coalesce(F.col(\"sex\"), F.lit(\"\")))) == \"male\", F.lit(\"Male\"))\n",
    "             .when(F.lower(F.trim(F.coalesce(F.col(\"sex\"), F.lit(\"\")))) == \"female\", F.lit(\"Female\"))\n",
    "             .when(F.lower(F.trim(F.coalesce(F.col(\"sex\"), F.lit(\"\")))) == \"other\", F.lit(\"Other\"))\n",
    "             .otherwise(F.initcap(F.trim(F.coalesce(F.col(\"sex\"), F.lit(None)))))\n",
    "        )\n",
    "\n",
    "        # build JSON payload for extras (if any), then drop raw extras columns\n",
    "        if extras:\n",
    "            selected = selected.withColumn(\"_raw_payload_json\", F.to_json(F.struct(*[F.col(c) for c in extras])))\n",
    "            selected = selected.drop(*extras)\n",
    "        else:\n",
    "            selected = selected.withColumn(\"_raw_payload_json\", F.lit(None).cast(StringType()))\n",
    "\n",
    "        # add ingestion metadata and compute row hash\n",
    "        selected = selected.withColumn(\"_ingestion_source_file\", F.lit(fname)) \\\n",
    "                           .withColumn(\"_ingestion_batch_id\", F.lit(ingestion_batch_id)) \\\n",
    "                           .withColumn(\"_ingestion_row_hash\", F.sha2(F.concat_ws(\"|\",\n",
    "                               F.coalesce(F.col(\"person_id\"), F.lit(\"\")),\n",
    "                               F.coalesce(F.col(\"household_id\"), F.lit(\"\")),\n",
    "                               F.coalesce(F.col(\"date_of_birth\").cast(StringType()), F.lit(\"\")),\n",
    "                               F.coalesce(F.col(\"last_updated\").cast(StringType()), F.lit(\"\"))\n",
    "                           ), 256))\n",
    "\n",
    "        conformed_list.append(selected)\n",
    "        processed_files.append({\"filename\": fname, \"status\": \"read_ok\", \"manifest_count\": row[\"manifest_reported_row_count\"]})\n",
    "    except Exception as e:\n",
    "        errors.append({\"filename\": fname, \"error\": str(e)})\n",
    "        # mark file failed in registry and continue processing other files\n",
    "        file_registry.update(\n",
    "            condition = F.expr(f\"filename = '{fname}'\"),\n",
    "            set = {\n",
    "                \"ingestion_status\": F.lit(\"Failed\"),\n",
    "                \"provenance_json\": F.concat(F.col(\"provenance_json\"), F.lit('\\n'), F.lit(json.dumps({\"error\": str(e)}))),\n",
    "                \"updated_at\": F.lit(datetime.utcnow())\n",
    "            }\n",
    "        )\n",
    "\n",
    "# If nothing succeeded\n",
    "if not conformed_list:\n",
    "    end_ts = datetime.utcnow()\n",
    "    spark.createDataFrame([(ingestion_batch_id, run_id, start_ts, end_ts, \"FAILED\", json.dumps({\"errors\": errors}))], schema=\"ingestion_batch_id string, run_id string, start_time timestamp, end_time timestamp, status string, notes string\").write.format(\"delta\").mode(\"append\").saveAsTable(INGESTION_AUDIT)\n",
    "    dbutils.notebook.exit(json.dumps({\"status\":\"FAIL\",\"reason\":\"no_files_ingested\",\"errors\":errors}))\n",
    "\n",
    "# union the conformed frames (allow missing columns since schema drift exists)\n",
    "union_df = conformed_list[0]\n",
    "for dfp in conformed_list[1:]:\n",
    "    union_df = union_df.unionByName(dfp, allowMissingColumns=True)\n",
    "\n",
    "# quick manifest vs observed total check (light)\n",
    "observed_total = union_df.count()\n",
    "manifest_total = sum([int(r[\"manifest_reported_row_count\"] or 0) for r in pending])\n",
    "\n",
    "if manifest_total == 0:\n",
    "    for r in pending:\n",
    "        file_registry.update(\n",
    "            condition = F.expr(f\"filename = '{r['filename']}'\"),\n",
    "            set = {\n",
    "                \"ingestion_status\": F.lit(\"Failed\"),\n",
    "                \"provenance_json\": F.concat(F.col(\"provenance_json\"), F.lit('\\n'), F.lit(\"manifest_total_zero\")),\n",
    "                \"updated_at\": F.lit(datetime.utcnow())\n",
    "            }\n",
    "        )\n",
    "    spark.createDataFrame([(ingestion_batch_id, run_id, start_ts, datetime.utcnow(), \"FAILED\", \"manifest_total_zero\")], schema=\"ingestion_batch_id string, run_id string, start_time timestamp, end_time timestamp, status string, notes string\").write.format(\"delta\").mode(\"append\").saveAsTable(INGESTION_AUDIT)\n",
    "    dbutils.notebook.exit(json.dumps({\"status\":\"FAIL\",\"reason\":\"manifest_total_zero\"}))\n",
    "\n",
    "pct_diff = abs(observed_total - manifest_total) / manifest_total\n",
    "\n",
    "# write to Bronze (partitioned by ingestion batch)\n",
    "union_df.write.format(\"delta\").mode(\"append\").partitionBy(\"_ingestion_batch_id\").saveAsTable(BRONZE_TABLE)\n",
    "\n",
    "# mark registry rows Succeeded\n",
    "for r in pending:\n",
    "    file_registry.update(\n",
    "        condition = F.expr(f\"filename = '{r['filename']}'\"),\n",
    "        set = {\n",
    "            \"ingestion_status\": F.lit(\"Succeeded\"),\n",
    "            \"updated_at\": F.lit(datetime.utcnow())\n",
    "        }\n",
    "    )\n",
    "\n",
    "end_ts = datetime.utcnow()\n",
    "spark.createDataFrame([(ingestion_batch_id, run_id, start_ts, end_ts, \"SUCCEEDED\", json.dumps({\"observed_total\": observed_total, \"manifest_total\": manifest_total, \"pct_diff\": pct_diff}))], schema=\"ingestion_batch_id string, run_id string, start_time timestamp, end_time timestamp, status string, notes string\").write.format(\"delta\").mode(\"append\").saveAsTable(INGESTION_AUDIT)\n",
    "\n",
    "dbutils.notebook.exit(json.dumps({\"status\":\"SUCCESS\",\"ingestion_batch_id\":ingestion_batch_id,\"observed_total\":observed_total,\"manifest_total\":manifest_total,\"pct_diff\":pct_diff}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2296e444-7fef-4545-9b82-ca487e95f05f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_ingest",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
