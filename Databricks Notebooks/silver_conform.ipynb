{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1aa7445a-8e68-4c1c-af84-4289b7fb076e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook: silver_conform\n",
    "# Path: /Workspace/Users/you/silver_conform\n",
    "#\n",
    "# Widgets:\n",
    "#  - ingestion_batch_id (optional)\n",
    "#  - census_year (optional)\n",
    "#\n",
    "# Output tables:\n",
    "#  - census.silver.dim_region\n",
    "#  - census.silver.dim_person\n",
    "#  - census.silver.dim_person_history\n",
    "#  - census.silver.dim_household\n",
    "#  - census.silver.lineage\n",
    "#  - census.silver.validation_reports_v1\n",
    "#\n",
    "# Behavior: idempotent, defensive, uses only DataFrame APIs and Delta MERGE/SQL.\n",
    "\n",
    "from datetime import datetime\n",
    "import json\n",
    "import uuid\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StringType\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "RAW_VOLUME_ROOT = \"/Volumes/census/raw/raw_files\"\n",
    "REGION_LOOKUP_PATH = f\"{RAW_VOLUME_ROOT}/region_lookup.parquet\"\n",
    "\n",
    "BRONZE_TABLE = \"census.bronze.individuals_raw_v1\"\n",
    "FILE_REG_TABLE = \"census.bronze.file_registry_v1\"\n",
    "SILVER_REGION = \"census.silver.dim_region\"\n",
    "SILVER_PERSON = \"census.silver.dim_person\"\n",
    "SILVER_PERSON_HISTORY = \"census.silver.dim_person_history\"\n",
    "SILVER_HOUSEHOLD = \"census.silver.dim_household\"\n",
    "SILVER_LINEAGE = \"census.silver.lineage\"\n",
    "SILVER_VALIDATION = \"census.silver.validation_reports_v1\"\n",
    "\n",
    "SOURCE_PRIORITY_MAP = {\n",
    "    \"AdminRegister\": 0,\n",
    "    \"FieldEnumeration\": 1,\n",
    "    \"SurveySample\": 2,\n",
    "    \"MobileUpdate\": 3\n",
    "}\n",
    "DEFAULT_SOURCE_PRIORITY = 99\n",
    "LEVENSHTEIN_THRESHOLD = 4\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def now():\n",
    "    return datetime.utcnow()\n",
    "\n",
    "def tidy_status_col(col):\n",
    "    return F.lower(F.trim(F.coalesce(F.col(col), F.lit(\"\"))))\n",
    "\n",
    "def pick_ingestion_batch():\n",
    "    # Accept widget if provided, else auto-select most-recent attempted batch\n",
    "    try:\n",
    "        widget_val = dbutils.widgets.get(\"ingestion_batch_id\")\n",
    "        if widget_val and widget_val.strip() != \"\":\n",
    "            return widget_val\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    if not spark.catalog.tableExists(FILE_REG_TABLE):\n",
    "        dbutils.notebook.exit(json.dumps({\"status\":\"FAIL\",\"reason\":\"missing_file_registry\",\"table\":FILE_REG_TABLE}))\n",
    "\n",
    "    fr = spark.table(FILE_REG_TABLE).withColumn(\"status_norm\", tidy_status_col(\"ingestion_status\"))\n",
    "    batches = fr.groupBy(\"ingestion_batch_id\").agg(\n",
    "        F.sum(F.coalesce(F.col(\"ingestion_attempts\"), F.lit(0))).alias(\"attempts\"),\n",
    "        F.sum(F.when(F.col(\"status_norm\") == \"succeeded\", 1).otherwise(0)).alias(\"succeeded_count\"),\n",
    "        F.max(\"updated_at\").alias(\"last_updated\")\n",
    "    ).filter(F.col(\"attempts\") > 0)\n",
    "\n",
    "    if batches.limit(1).count() == 0:\n",
    "        dbutils.notebook.exit(json.dumps({\"status\":\"FAIL\",\"reason\":\"no_ingestion_attempts_found\",\"message\":\"Run Bronze ingest first or provide ingestion_batch_id widget\"}))\n",
    "\n",
    "    candidate = batches.orderBy(F.desc(\"succeeded_count\"), F.desc(\"last_updated\")).first()\n",
    "    return candidate[\"ingestion_batch_id\"]\n",
    "\n",
    "# ---------- start ----------\n",
    "ingestion_batch_id = pick_ingestion_batch()\n",
    "try:\n",
    "    census_year_widget = dbutils.widgets.get(\"census_year\")\n",
    "    census_year = int(census_year_widget) if census_year_widget and census_year_widget.strip() != \"\" else None\n",
    "except Exception:\n",
    "    census_year = None\n",
    "\n",
    "start_ts = now()\n",
    "run_id = f\"silver_conform-{ingestion_batch_id}-{start_ts.strftime('%Y%m%dT%H%M%SZ')}\"\n",
    "print(f\"silver_conform starting. batch={ingestion_batch_id}, census_year={census_year}, run_id={run_id}\")\n",
    "\n",
    "# ---------- Defensive canonical region load (replace prior region load block) ----------\n",
    "try:\n",
    "    region_src = spark.read.format(\"parquet\").load(REGION_LOOKUP_PATH)\n",
    "    src_cols = set(region_src.columns)\n",
    "\n",
    "    # pick the best id column safely (only reference existing columns)\n",
    "    if \"geoid\" in src_cols:\n",
    "        id_col_expr = F.col(\"geoid\").cast(\"int\")\n",
    "    elif \"geo_id\" in src_cols:\n",
    "        id_col_expr = F.col(\"geo_id\").cast(\"int\")\n",
    "    elif \"region_id\" in src_cols:\n",
    "        id_col_expr = F.col(\"region_id\").cast(\"int\")\n",
    "    elif \"id\" in src_cols:\n",
    "        id_col_expr = F.col(\"id\").cast(\"int\")\n",
    "    else:\n",
    "        # final fallback: null geoid (should not be the normal case)\n",
    "        id_col_expr = F.lit(None).cast(\"int\").alias(\"geoid\")\n",
    "\n",
    "    # pick iso code column safely\n",
    "    if \"iso_admin_code\" in src_cols:\n",
    "        iso_expr = F.col(\"iso_admin_code\")\n",
    "    elif \"iso_code\" in src_cols:\n",
    "        iso_expr = F.col(\"iso_code\")\n",
    "    else:\n",
    "        iso_expr = F.lit(\"\")\n",
    "\n",
    "    # pick region name safely\n",
    "    if \"region_name_standard\" in src_cols:\n",
    "        name_expr = F.col(\"region_name_standard\")\n",
    "    elif \"region_name\" in src_cols:\n",
    "        name_expr = F.col(\"region_name\")\n",
    "    elif \"name\" in src_cols:\n",
    "        name_expr = F.col(\"name\")\n",
    "    else:\n",
    "        name_expr = F.lit(\"\")\n",
    "\n",
    "    # pick parent geoid if present\n",
    "    parent_expr = F.col(\"parent_geoid\").cast(\"int\") if \"parent_geoid\" in src_cols else F.lit(0).cast(\"int\")\n",
    "\n",
    "    # pick urban/rural flag\n",
    "    urban_expr = F.col(\"urban_rural_flag\") if \"urban_rural_flag\" in src_cols else F.lit(\"\")\n",
    "\n",
    "    # Construct centroid defensively:\n",
    "    # If a struct named 'centroid' exists normalize its nested fields;\n",
    "    # else find scalar lat/lon candidates and create a struct; otherwise produce null-lat/lon struct.\n",
    "    if \"centroid\" in src_cols:\n",
    "        region_work = region_src.withColumn(\"centroid\", F.col(\"centroid\"))\n",
    "    else:\n",
    "        # common scalar name candidates\n",
    "        lat_candidates = [\"latitude_center\", \"latitude\", \"lat\", \"centroid_latitude\", \"centroid_lat\"]\n",
    "        lon_candidates = [\"longitude_center\", \"longitude\", \"lon\", \"lng\", \"centroid_longitude\", \"centroid_lon\"]\n",
    "        found_lat = next((c for c in lat_candidates if c in src_cols), None)\n",
    "        found_lon = next((c for c in lon_candidates if c in src_cols), None)\n",
    "        if found_lat and found_lon:\n",
    "            region_work = region_src.withColumn(\"centroid\", F.struct(\n",
    "                F.col(found_lat).cast(\"double\").alias(\"latitude\"),\n",
    "                F.col(found_lon).cast(\"double\").alias(\"longitude\")\n",
    "            ))\n",
    "        else:\n",
    "            # try fuzzy name detection if explicit candidates not found\n",
    "            lat_guess = next((c for c in src_cols if \"lat\" in c.lower() and c.lower() != \"latitude\"), None)\n",
    "            lon_guess = next((c for c in src_cols if (\"lon\" in c.lower() or \"lng\" in c.lower()) and c != lat_guess), None)\n",
    "            if lat_guess and lon_guess:\n",
    "                region_work = region_src.withColumn(\"centroid\", F.struct(\n",
    "                    F.col(lat_guess).cast(\"double\").alias(\"latitude\"),\n",
    "                    F.col(lon_guess).cast(\"double\").alias(\"longitude\")\n",
    "                ))\n",
    "            else:\n",
    "                region_work = region_src.withColumn(\"centroid\", F.struct(\n",
    "                    F.lit(None).cast(\"double\").alias(\"latitude\"),\n",
    "                    F.lit(None).cast(\"double\").alias(\"longitude\")\n",
    "                ))\n",
    "\n",
    "    # Project canonical schema using only expressions built from existing columns\n",
    "    region_df = region_work.select(\n",
    "        id_col_expr.alias(\"geoid\"),\n",
    "        iso_expr.alias(\"iso_admin_code\"),\n",
    "        name_expr.alias(\"region_name_standard\"),\n",
    "        parent_expr.alias(\"parent_geoid\"),\n",
    "        urban_expr.alias(\"urban_rural_flag\"),\n",
    "        F.col(\"centroid\").alias(\"centroid\")\n",
    "    )\n",
    "\n",
    "    # Normalize centroid nested keys to (latitude, longitude) if needed\n",
    "    region_df = region_df.withColumn(\n",
    "        \"centroid\",\n",
    "        F.struct(\n",
    "            F.col(\"centroid.latitude\").cast(\"double\").alias(\"latitude\"),\n",
    "            F.col(\"centroid.longitude\").cast(\"double\").alias(\"longitude\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Final write: overwrite canonical dim_region\n",
    "    region_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(SILVER_REGION)\n",
    "    region_count = region_df.count()\n",
    "    print(f\"dim_region written ({region_count} rows).\")\n",
    "except Exception as e:\n",
    "    dbutils.notebook.exit(json.dumps({\"status\":\"FAIL\",\"reason\":\"region_load_failed\",\"error\":str(e)}))\n",
    "\n",
    "# ---------- 2) Read Bronze partition for batch ----------\n",
    "if not spark.catalog.tableExists(BRONZE_TABLE):\n",
    "    dbutils.notebook.exit(json.dumps({\"status\":\"FAIL\",\"reason\":\"missing_bronze_table\"}))\n",
    "\n",
    "bronze_df = spark.table(BRONZE_TABLE).filter(F.col(\"_ingestion_batch_id\") == ingestion_batch_id)\n",
    "if census_year:\n",
    "    bronze_df = bronze_df.filter(F.col(\"census_year\") == int(census_year))\n",
    "\n",
    "bronze_count = bronze_df.count()\n",
    "if bronze_count == 0:\n",
    "    dbutils.notebook.exit(json.dumps({\"status\":\"FAIL\",\"reason\":\"no_bronze_rows_for_batch\",\"ingestion_batch_id\":ingestion_batch_id}))\n",
    "\n",
    "print(f\"bronze rows: {bronze_count}\")\n",
    "\n",
    "# ---------- 3) Prepare canonical projection from Bronze ----------\n",
    "canonical_cols = {\n",
    "    \"person_id\": [\"person_id\"],\n",
    "    \"national_id\": [\"national_id\"],\n",
    "    \"household_id\": [\"household_id\"],\n",
    "    \"geoid\": [\"geoid\"],\n",
    "    \"region_name_reported\": [\"region_name_reported\"],\n",
    "    \"census_year\": [\"census_year\"],\n",
    "    \"date_of_birth\": [\"date_of_birth\"],\n",
    "    \"age\": [\"age\"],\n",
    "    \"sex\": [\"sex\"],\n",
    "    \"education_level\": [\"education_level\"],\n",
    "    \"literacy\": [\"literacy\"],\n",
    "    \"employment_status\": [\"employment_status\"],\n",
    "    \"employment_type\": [\"employment_type\"],\n",
    "    \"industry_code\": [\"industry_code\"],\n",
    "    \"annual_income_local\": [\"annual_income_local\"],\n",
    "    \"record_confidence_score\": [\"record_confidence_score\"],\n",
    "    \"last_updated\": [\"last_updated\"],\n",
    "    \"_ingestion_row_hash\": [\"_ingestion_row_hash\"],\n",
    "    \"_ingestion_source_file\": [\"_ingestion_source_file\"],\n",
    "    \"_ingestion_batch_id\": [\"_ingestion_batch_id\"],\n",
    "    \"enumeration_source\": [\"enumeration_source\"],\n",
    "    \"_raw_payload_json\": [\"_raw_payload_json\"]\n",
    "}\n",
    "\n",
    "bronze_cols = set(bronze_df.columns)\n",
    "exprs = []\n",
    "for canon, variants in canonical_cols.items():\n",
    "    chosen = next((v for v in variants if v in bronze_cols), None)\n",
    "    if chosen:\n",
    "        if canon in (\"geoid\", \"age\"):\n",
    "            exprs.append(F.col(chosen).cast(\"int\").alias(canon))\n",
    "        elif canon == \"annual_income_local\":\n",
    "            exprs.append(F.col(chosen).cast(\"double\").alias(canon))\n",
    "        else:\n",
    "            exprs.append(F.col(chosen).alias(canon))\n",
    "    else:\n",
    "        exprs.append(F.lit(None).alias(canon))\n",
    "\n",
    "selected = bronze_df.select(*exprs)\n",
    "\n",
    "# normalize sex\n",
    "selected = selected.withColumn(\"sex\",\n",
    "    F.when(F.lower(F.trim(F.coalesce(F.col(\"sex\"), F.lit(\"\")))) == \"male\", F.lit(\"Male\"))\n",
    "     .when(F.lower(F.trim(F.coalesce(F.col(\"sex\"), F.lit(\"\")))) == \"female\", F.lit(\"Female\"))\n",
    "     .when(F.lower(F.trim(F.coalesce(F.col(\"sex\"), F.lit(\"\")))) == \"other\", F.lit(\"Other\"))\n",
    "     .otherwise(F.initcap(F.trim(F.coalesce(F.col(\"sex\"), F.lit(None)))))\n",
    ")\n",
    "\n",
    "# ensure _ingestion_row_hash exists\n",
    "selected = selected.withColumn(\"_ingestion_row_hash\", F.coalesce(F.col(\"_ingestion_row_hash\"),\n",
    "    F.sha2(F.concat_ws(\"|\", F.coalesce(F.col(\"person_id\"),F.lit(\"\")),\n",
    "                         F.coalesce(F.col(\"household_id\"),F.lit(\"\")),\n",
    "                         F.coalesce(F.col(\"date_of_birth\").cast(StringType()),F.lit(\"\")),\n",
    "                         F.coalesce(F.col(\"last_updated\").cast(StringType()),F.lit(\"\"))\n",
    "                        ), 256)))\n",
    "\n",
    "# ---------- 4) Region reconciliation ----------\n",
    "regions = spark.table(SILVER_REGION).select(\"geoid\",\"region_name_standard\").withColumn(\"region_std_norm\", F.lower(F.trim(F.regexp_replace(F.coalesce(F.col(\"region_name_standard\"), F.lit(\"\")), r\"[^a-zA-Z0-9\\s]\",\" \"))))\n",
    "bronze_recon = selected.withColumn(\"region_norm\", F.lower(F.trim(F.regexp_replace(F.coalesce(F.col(\"region_name_reported\"), F.lit(\"\")), r\"[^a-zA-Z0-9\\s]\",\" \"))))\n",
    "\n",
    "# exact normalized match\n",
    "exact = bronze_recon.join(regions, bronze_recon.region_norm == regions.region_std_norm, how=\"left\").select(bronze_recon[\"*\"], regions[\"geoid\"].alias(\"recon_geoid_exact\"))\n",
    "\n",
    "# fuzzy fallback using levenshtein (safe because regions small)\n",
    "regions_norm = regions.select(\"geoid\",\"region_std_norm\").withColumnRenamed(\"region_std_norm\",\"r_std_norm\")\n",
    "lev = bronze_recon.alias(\"b\").crossJoin(regions_norm.alias(\"r\")) \\\n",
    "    .select(F.col(\"b.*\"), F.col(\"r.geoid\").alias(\"candidate_geoid\"), F.levenshtein(F.col(\"b.region_norm\"), F.col(\"r.r_std_norm\")).alias(\"lev\")) \\\n",
    "    .filter(F.col(\"lev\") <= LEVENSHTEIN_THRESHOLD)\n",
    "\n",
    "w = Window.partitionBy(\"_ingestion_row_hash\").orderBy(F.asc(\"lev\"))\n",
    "lev_best = lev.withColumn(\"rn\", F.row_number().over(w)).filter(F.col(\"rn\")==1).select(\"_ingestion_row_hash\",\"candidate_geoid\",\"lev\").withColumnRenamed(\"candidate_geoid\",\"recon_geoid_lev\")\n",
    "\n",
    "recon = exact.join(lev_best, on=\"_ingestion_row_hash\", how=\"left\")\n",
    "recon_resolved = recon.withColumn(\"geoid_resolved\", F.coalesce(F.col(\"geoid\"), F.col(\"recon_geoid_exact\"), F.col(\"recon_geoid_lev\"))) \\\n",
    "    .withColumn(\"region_recon_flag\", F.when(F.col(\"geoid_resolved\").isNull(), F.lit(True)).otherwise(F.lit(False))) \\\n",
    "    .drop(\"geoid\").withColumnRenamed(\"geoid_resolved\",\"geoid\")\n",
    "\n",
    "# ---------- 5) Deterministic grouping & canonical selection ----------\n",
    "cand = recon_resolved.withColumn(\"match_key_national\", F.when(F.col(\"national_id\").isNotNull() & (F.col(\"national_id\") != \"\"), F.col(\"national_id\")).otherwise(F.lit(None))) \\\n",
    "    .withColumn(\"match_key_composite\", F.concat_ws(\"|\", F.coalesce(F.col(\"household_id\"),F.lit(\"\")),\n",
    "                                                     F.coalesce(F.col(\"date_of_birth\").cast(StringType()),F.lit(\"\")),\n",
    "                                                     F.coalesce(F.col(\"geoid\").cast(StringType()),F.lit(\"\"))\n",
    "                                                    )) \\\n",
    "    .withColumn(\"source_priority\",\n",
    "                F.when(F.col(\"enumeration_source\") == \"AdminRegister\", 0)\n",
    "                 .when(F.col(\"enumeration_source\") == \"FieldEnumeration\", 1)\n",
    "                 .when(F.col(\"enumeration_source\") == \"SurveySample\", 2)\n",
    "                 .when(F.col(\"enumeration_source\") == \"MobileUpdate\", 3)\n",
    "                 .otherwise(F.lit(DEFAULT_SOURCE_PRIORITY))\n",
    "               )\n",
    "\n",
    "cand = cand.withColumn(\"census_year\", F.col(\"census_year\").cast(\"int\"))\n",
    "cand = cand.withColumn(\"group_key\",\n",
    "                      F.when(F.col(\"match_key_national\").isNotNull(), F.concat_ws(\"|\", F.col(\"match_key_national\"), F.col(\"census_year\").cast(StringType())))\n",
    "                       .otherwise(F.concat_ws(\"|\", F.col(\"match_key_composite\"), F.col(\"census_year\").cast(StringType())))\n",
    "                     )\n",
    "\n",
    "w2 = Window.partitionBy(\"group_key\").orderBy(F.asc(\"source_priority\"), F.desc(F.coalesce(F.col(\"record_confidence_score\"), F.lit(0.0))), F.desc(F.coalesce(F.col(\"last_updated\"), F.lit('1970-01-01'))))\n",
    "ranked = cand.withColumn(\"rank\", F.row_number().over(w2))\n",
    "canonical = ranked.filter(F.col(\"rank\") == 1)\n",
    "\n",
    "grouped = cand.groupBy(\"group_key\",\"census_year\").agg(\n",
    "    F.collect_list(F.col(\"_ingestion_row_hash\")).alias(\"merged_from_bronze\"),\n",
    "    F.collect_set(F.col(\"_ingestion_source_file\")).alias(\"contributing_files\"),\n",
    "    F.min(\"source_priority\").alias(\"min_source_priority\")\n",
    ")\n",
    "\n",
    "canonical_attrs = canonical.select(\"group_key\",\"person_id\",\"national_id\",\"household_id\",\"geoid\",\"date_of_birth\",\"age\",\"sex\",\"education_level\",\"literacy\",\"employment_status\",\"employment_type\",\"annual_income_local\",\"record_confidence_score\",\"last_updated\",\"enumeration_source\")\n",
    "\n",
    "silver_prep = grouped.join(canonical_attrs, on=\"group_key\", how=\"left\") \\\n",
    "    .withColumn(\"canonical_person_id\", F.coalesce(F.col(\"national_id\"), F.sha2(F.col(\"group_key\"),256))) \\\n",
    "    .withColumn(\"person_surrogate_id\", F.sha2(F.concat_ws(\"|\", F.coalesce(F.col(\"canonical_person_id\"),F.col(\"group_key\")), F.lit(run_id)),256)) \\\n",
    "    .withColumn(\"ingestion_batch_id\", F.lit(ingestion_batch_id)) \\\n",
    "    .withColumn(\"is_current\", F.lit(True)) \\\n",
    "    .withColumn(\"effective_from\", F.current_timestamp()) \\\n",
    "    .withColumn(\"effective_to\", F.lit(None).cast(\"timestamp\")) \\\n",
    "    .withColumn(\"source_priority\", F.col(\"min_source_priority\"))\n",
    "\n",
    "person_final = silver_prep.select(\n",
    "    \"person_surrogate_id\",\"canonical_person_id\",\"group_key\",\"census_year\",\"person_id\",\"national_id\",\"household_id\",\"geoid\",\"date_of_birth\",\"age\",\"sex\",\"education_level\",\"literacy\",\"employment_status\",\"employment_type\",\"annual_income_local\",\"record_confidence_score\",\"last_updated\",\"merged_from_bronze\",\"contributing_files\",\"source_priority\",\"ingestion_batch_id\",\"is_current\",\"effective_from\",\"effective_to\"\n",
    ")\n",
    "\n",
    "\n",
    "print(\"DEBUG: person_final pre-upsert count:\", person_final.count())\n",
    "display(person_final.select(\"canonical_person_id\",\"census_year\",\"ingestion_batch_id\").limit(10))\n",
    "\n",
    "print(\"DEBUG: Incoming batch distinct canonical_person_id count:\", person_final.select(\"canonical_person_id\").distinct().count())\n",
    "print(\"DEBUG: Sample incoming keys:\", person_final.select(\"canonical_person_id\", \"census_year\").limit(5).collect())\n",
    "\n",
    "# ---------- 6) SCD2 upsert into dim_person with proper history ----------\n",
    "def scd2_upsert_with_history(target_table, history_table, incoming_df):\n",
    "    \"\"\"\n",
    "    SCD2 upsert that properly captures historical versions.\n",
    "    IMPORTANT: Only triggers on attribute changes, NOT on batch_id change alone.\n",
    "    \"\"\"\n",
    "    tmp_view = f\"tmp_in_{uuid.uuid4().hex}\"\n",
    "    incoming_df.createOrReplaceTempView(tmp_view)\n",
    "    \n",
    "    # Attributes compared for detecting changes - ONLY these should trigger SCD2\n",
    "    attr_cols = [\"age\", \"sex\", \"education_level\", \"literacy\", \"employment_status\",\n",
    "                 \"employment_type\", \"annual_income_local\", \"household_id\", \"geoid\",\n",
    "                 \"date_of_birth\", \"record_confidence_score\"]\n",
    "    \n",
    "    # Build SQL list for comparison - use IS NOT DISTINCT FROM for proper NULL handling\n",
    "    diff_cond_parts = []\n",
    "    for c in attr_cols:\n",
    "        # Handle both NULL values and actual differences\n",
    "        diff_cond_parts.append(f\"(NOT (t.{c} IS NOT DISTINCT FROM s.{c}))\")\n",
    "    \n",
    "    # Only trigger SCD2 when attributes actually change\n",
    "    diff_cond = \" OR \".join(diff_cond_parts) if diff_cond_parts else \"false\"\n",
    "    \n",
    "    print(f\"DEBUG: Using diff condition (ONLY attribute changes): {diff_cond}\")\n",
    "    print(f\"DEBUG: Batch ID alone will NOT trigger SCD2\")\n",
    "    \n",
    "    # 1) Capture rows that will be expired (for history table)\n",
    "    # These are rows where attributes changed AND they're currently current\n",
    "    capture_expired_sql = f\"\"\"\n",
    "    SELECT t.*\n",
    "    FROM {target_table} t\n",
    "    INNER JOIN {tmp_view} s\n",
    "      ON t.canonical_person_id = s.canonical_person_id\n",
    "     AND t.census_year = s.census_year\n",
    "    WHERE t.is_current = true\n",
    "      AND ({diff_cond})\n",
    "    \"\"\"\n",
    "    \n",
    "    expired_rows = spark.sql(capture_expired_sql)\n",
    "    \n",
    "    if expired_rows.count() > 0:\n",
    "        print(f\"DEBUG: Capturing {expired_rows.count()} expired rows for history (attributes changed)\")\n",
    "        # Mark them as expired with current timestamp\n",
    "        expired_for_history = expired_rows.withColumn(\"is_current\", F.lit(False)) \\\n",
    "                                          .withColumn(\"effective_to\", F.current_timestamp())\n",
    "        # Append to history table\n",
    "        expired_for_history.write.format(\"delta\").mode(\"append\").saveAsTable(history_table)\n",
    "        print(f\"DEBUG: Appended {expired_for_history.count()} rows to history table\")\n",
    "    else:\n",
    "        print(f\"DEBUG: No attribute changes detected - no rows to expire\")\n",
    "    \n",
    "    # 2) MERGE: Expire existing current rows ONLY if attributes changed\n",
    "    expire_merge_sql = f\"\"\"\n",
    "    MERGE INTO {target_table} t\n",
    "    USING (SELECT * FROM {tmp_view}) s\n",
    "    ON t.canonical_person_id = s.canonical_person_id\n",
    "      AND t.census_year = s.census_year\n",
    "      AND t.is_current = true\n",
    "    WHEN MATCHED AND ({diff_cond})\n",
    "      THEN UPDATE SET \n",
    "        t.is_current = false, \n",
    "        t.effective_to = current_timestamp()\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        expire_result = spark.sql(expire_merge_sql)\n",
    "        expired_count = expire_result.count() if hasattr(expire_result, 'count') else 0\n",
    "        print(f\"DEBUG: Expired {expired_count} rows via MERGE (attribute changes)\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Expire MERGE failed: {e}\")\n",
    "        raise\n",
    "\n",
    "    # 3) MERGE: Insert new rows or update existing\n",
    "    # For matching rows: always update batch_id and other metadata, but only insert new version if attributes changed\n",
    "    insert_merge_sql = f\"\"\"\n",
    "    MERGE INTO {target_table} t\n",
    "    USING (SELECT * FROM {tmp_view}) s\n",
    "    ON t.canonical_person_id = s.canonical_person_id\n",
    "      AND t.census_year = s.census_year\n",
    "      AND t.is_current = true\n",
    "    WHEN MATCHED AND ({diff_cond})\n",
    "      THEN UPDATE SET\n",
    "        t.person_surrogate_id = s.person_surrogate_id,\n",
    "        t.person_id = s.person_id,\n",
    "        t.national_id = s.national_id,\n",
    "        t.household_id = s.household_id,\n",
    "        t.geoid = s.geoid,\n",
    "        t.date_of_birth = s.date_of_birth,\n",
    "        t.age = s.age,\n",
    "        t.sex = s.sex,\n",
    "        t.education_level = s.education_level,\n",
    "        t.literacy = s.literacy,\n",
    "        t.employment_status = s.employment_status,\n",
    "        t.employment_type = s.employment_type,\n",
    "        t.annual_income_local = s.annual_income_local,\n",
    "        t.record_confidence_score = s.record_confidence_score,\n",
    "        t.last_updated = s.last_updated,\n",
    "        t.merged_from_bronze = s.merged_from_bronze,\n",
    "        t.contributing_files = s.contributing_files,\n",
    "        t.source_priority = s.source_priority,\n",
    "        t.ingestion_batch_id = s.ingestion_batch_id,\n",
    "        t.is_current = true,\n",
    "        t.effective_from = current_timestamp(),\n",
    "        t.effective_to = NULL\n",
    "    WHEN NOT MATCHED\n",
    "      THEN INSERT *\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        spark.sql(insert_merge_sql)\n",
    "        print(f\"DEBUG: Insert/Update MERGE completed\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"MERGE upsert failed for {target_table}: {e}\")\n",
    "    \n",
    "    # 4) Clean up expired rows from dim_person (keep only current rows)\n",
    "    delete_sql = f\"\"\"\n",
    "    DELETE FROM {target_table}\n",
    "    WHERE is_current = false\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        delete_result = spark.sql(delete_sql)\n",
    "        deleted_count = delete_result.count() if hasattr(delete_result, 'count') else 0\n",
    "        print(f\"DEBUG: Deleted {deleted_count} expired rows from {target_table}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] DELETE failed: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # 5) For rows where attributes didn't change, update the batch_id\n",
    "    # This handles the case where same data comes in different batch\n",
    "    update_batch_only_sql = f\"\"\"\n",
    "    MERGE INTO {target_table} t\n",
    "    USING (SELECT * FROM {tmp_view}) s\n",
    "    ON t.canonical_person_id = s.canonical_person_id\n",
    "      AND t.census_year = s.census_year\n",
    "      AND t.is_current = true\n",
    "    WHEN MATCHED AND (NOT ({diff_cond}))\n",
    "      THEN UPDATE SET\n",
    "        t.ingestion_batch_id = s.ingestion_batch_id,\n",
    "        t.merged_from_bronze = s.merged_from_bronze,\n",
    "        t.contributing_files = s.contributing_files\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        update_result = spark.sql(update_batch_only_sql)\n",
    "        updated_count = update_result.count() if hasattr(update_result, 'count') else 0\n",
    "        print(f\"DEBUG: Updated batch_id only for {updated_count} rows (no attribute changes)\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARNING] Batch-only update failed (non-critical): {e}\")\n",
    "\n",
    "        \n",
    "# ---------- ENSURE SCHEMA + TARGET TABLES EXIST (defensive bootstrap) ----------\n",
    "# Make sure the Unity Catalog schema exists (idempotent)\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS census.silver\")\n",
    "\n",
    "# If dim_person does not exist, create an empty Delta table with the same schema\n",
    "if not spark.catalog.tableExists(SILVER_PERSON):\n",
    "    empty_person_df = spark.createDataFrame([], schema=person_final.schema)\n",
    "    empty_person_df.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"geoid\").saveAsTable(SILVER_PERSON)\n",
    "    print(\"Bootstrapped empty table:\", SILVER_PERSON)\n",
    "else:\n",
    "    print(\"Target table already exists:\", SILVER_PERSON)\n",
    "\n",
    "# If dim_person_history does not exist, create it with EXACTLY the same schema\n",
    "if not spark.catalog.tableExists(SILVER_PERSON_HISTORY):\n",
    "    # Create history table with same schema as person_final\n",
    "    empty_history_df = spark.createDataFrame([], schema=person_final.schema)\n",
    "    empty_history_df.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"geoid\").saveAsTable(SILVER_PERSON_HISTORY)\n",
    "    print(\"Bootstrapped empty table:\", SILVER_PERSON_HISTORY)\n",
    "\n",
    "pre_total = spark.table(SILVER_PERSON).count()\n",
    "pre_batch_cnt = spark.table(SILVER_PERSON).filter(F.col(\"ingestion_batch_id\")==ingestion_batch_id).count()\n",
    "print(f\"DEBUG: dim_person pre-merge total={pre_total}, batch_count={pre_batch_cnt}\")\n",
    "\n",
    "# Call the SCD2 upsert function\n",
    "scd2_upsert_with_history(SILVER_PERSON, SILVER_PERSON_HISTORY, person_final)\n",
    "\n",
    "# Post-merge counts\n",
    "post_total = spark.table(SILVER_PERSON).count()\n",
    "post_batch_cnt = spark.table(SILVER_PERSON).filter(F.col(\"ingestion_batch_id\")==ingestion_batch_id).count()\n",
    "history_total = spark.table(SILVER_PERSON_HISTORY).count()\n",
    "print(f\"DEBUG: dim_person post-merge total={post_total}, batch_count={post_batch_cnt}\")\n",
    "print(f\"DEBUG: dim_person_history total={history_total}\")\n",
    "print(f\"DEBUG: Current vs expired ratio: {post_total} current, {history_total - pre_total if history_total > pre_total else 0} new historical rows\")\n",
    "\n",
    "# Verify that dim_person only has current rows\n",
    "current_check = spark.table(SILVER_PERSON).filter(F.col(\"is_current\") == False).count()\n",
    "if current_check > 0:\n",
    "    print(f\"WARNING: Found {current_check} non-current rows in dim_person. They should have been deleted.\")\n",
    "\n",
    "# ---------- 7) Household aggregates ----------\n",
    "# explode merged_from_bronze and join back to Bronze to compute household-level metrics\n",
    "contrib_flat = silver_prep.select(\"group_key\", F.explode(F.col(\"merged_from_bronze\")).alias(\"bronze_hash\"))\n",
    "bronze_key = bronze_df.select(\"_ingestion_row_hash\",\"household_id\",\"annual_income_local\",\"geoid\",\"literacy\")\n",
    "household_joined = contrib_flat.join(bronze_key, contrib_flat.bronze_hash == bronze_key._ingestion_row_hash, how=\"left\").select(\"household_id\",\"annual_income_local\",\"geoid\",\"literacy\")\n",
    "household_agg = household_joined.groupBy(\"household_id\",\"geoid\").agg(\n",
    "    F.count(\"*\").alias(\"household_size\"),\n",
    "    F.expr(\"percentile_approx(annual_income_local, 0.5)\").alias(\"median_household_income\"),\n",
    "    F.avg(F.when(F.col(\"literacy\")==True, 1).otherwise(0)).alias(\"household_literacy_rate\")\n",
    ").withColumn(\"ingestion_batch_id\", F.lit(ingestion_batch_id))\n",
    "\n",
    "try:\n",
    "    tgt_house = DeltaTable.forName(spark, SILVER_HOUSEHOLD)\n",
    "    tgt_house.alias(\"t\").merge(\n",
    "        household_agg.alias(\"s\"),\n",
    "        \"t.household_id = s.household_id AND t.geoid = s.geoid\"\n",
    "    ).whenMatchedUpdate(set={\n",
    "        \"household_size\": F.col(\"s.household_size\"),\n",
    "        \"median_household_income\": F.col(\"s.median_household_income\"),\n",
    "        \"household_literacy_rate\": F.col(\"s.household_literacy_rate\"),\n",
    "        \"ingestion_batch_id\": F.col(\"s.ingestion_batch_id\")\n",
    "    }).whenNotMatchedInsertAll().execute()\n",
    "except Exception:\n",
    "    household_agg.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"geoid\").saveAsTable(SILVER_HOUSEHOLD)\n",
    "\n",
    "# ---------- 8) Lineage table ----------\n",
    "lineage_df = person_final.select(\"canonical_person_id\",\"person_surrogate_id\",\"census_year\",\"merged_from_bronze\",\"contributing_files\",\"ingestion_batch_id\").dropDuplicates([\"canonical_person_id\",\"census_year\",\"ingestion_batch_id\"])\n",
    "try:\n",
    "    tgt_lineage = DeltaTable.forName(spark, SILVER_LINEAGE)\n",
    "    tgt_lineage.alias(\"t\").merge(\n",
    "        lineage_df.alias(\"s\"),\n",
    "        \"t.canonical_person_id = s.canonical_person_id AND t.census_year = s.census_year AND t.ingestion_batch_id = s.ingestion_batch_id\"\n",
    "    ).whenNotMatchedInsertAll().execute()\n",
    "except Exception:\n",
    "    lineage_df.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"ingestion_batch_id\").saveAsTable(SILVER_LINEAGE)\n",
    "\n",
    "# ---------- 9) Final summary & validation row ----------\n",
    "end_ts = now()\n",
    "person_count = spark.table(SILVER_PERSON).filter(F.col(\"ingestion_batch_id\") == ingestion_batch_id).count() if spark.catalog.tableExists(SILVER_PERSON) else 0\n",
    "hh_count = spark.table(SILVER_HOUSEHOLD).filter(F.col(\"ingestion_batch_id\") == ingestion_batch_id).count() if spark.catalog.tableExists(SILVER_HOUSEHOLD) else 0\n",
    "lineage_count = spark.table(SILVER_LINEAGE).filter(F.col(\"ingestion_batch_id\") == ingestion_batch_id).count() if spark.catalog.tableExists(SILVER_LINEAGE) else 0\n",
    "\n",
    "report = {\n",
    "    \"ingestion_batch_id\": ingestion_batch_id,\n",
    "    \"run_id\": run_id,\n",
    "    \"start_time\": start_ts.isoformat(),\n",
    "    \"end_time\": end_ts.isoformat(),\n",
    "    \"bronze_row_count\": bronze_count,\n",
    "    \"person_count\": person_count,\n",
    "    \"household_count\": hh_count,\n",
    "    \"lineage_count\": lineage_count,\n",
    "    \"status\": \"SUCCEEDED\"\n",
    "}\n",
    "\n",
    "spark.createDataFrame([(ingestion_batch_id, run_id, datetime.utcnow(), json.dumps(report), \"PASS\")], schema=\"ingestion_batch_id string, run_id string, report_time timestamp, report_json string, status string\").write.format(\"delta\").mode(\"append\").saveAsTable(SILVER_VALIDATION)\n",
    "\n",
    "dbutils.notebook.exit(json.dumps({\"status\":\"SUCCESS\",\"report\":report}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "401ab002-ddb1-4e10-92fc-3cdea6e48d7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_conform",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}