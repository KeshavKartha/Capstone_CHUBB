{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d1429ae-8d9b-4e6c-8512-9e4108be9907",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spark-12934f0d-5336-4e00-b1d7-97/.ipykernel/2520/command-6278575085301030-2781355552:22: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  INGESTION_BATCH_ID = f\"reg-{datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')}\"\n/home/spark-12934f0d-5336-4e00-b1d7-97/.ipykernel/2520/command-6278575085301030-2781355552:143: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  \"created_at\": datetime.utcnow(),\n/home/spark-12934f0d-5336-4e00-b1d7-97/.ipykernel/2520/command-6278575085301030-2781355552:144: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  \"updated_at\": datetime.utcnow(),\n/home/spark-12934f0d-5336-4e00-b1d7-97/.ipykernel/2520/command-6278575085301030-2781355552:211: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  \"timestamp_utc\": datetime.utcnow().isoformat(),\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 967 bytes.\n✔ Registration completed: 4 files\n✔ Report written to: /Volumes/census/raw/raw_files/registration_reports/registration_report_reg-20260106T090523Z.json\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import hashlib\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import (\n",
    "    StringType, LongType, TimestampType, StructType, StructField\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "RAW_VOLUME_ROOT = \"/Volumes/census/raw/raw_files\"\n",
    "MANIFEST_PATH = f\"{RAW_VOLUME_ROOT}/manifest.json\"\n",
    "\n",
    "CATALOG = \"census\"\n",
    "SCHEMA = \"bronze\"\n",
    "TABLE = \"file_registry_v1\"\n",
    "FULL_TABLE = f\"{CATALOG}.{SCHEMA}.{TABLE}\"\n",
    "\n",
    "QUICK_COUNT_MAX_BYTES = 5 * 1024 * 1024\n",
    "INGESTION_BATCH_ID = f\"reg-{datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')}\"\n",
    "\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "# ============================================================\n",
    "# HELPERS\n",
    "# ============================================================\n",
    "\n",
    "def parse_manifest_row_count(entry):\n",
    "    for key in (\"row_count\", \"rows\", \"rowCount\"):\n",
    "        v = entry.get(key)\n",
    "        if v is None:\n",
    "            continue\n",
    "        try:\n",
    "            return int(v)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "\n",
    "def estimate_csv_rows(path, file_size):\n",
    "    try:\n",
    "        raw = dbutils.fs.head(path, 1_000_000)\n",
    "        if not raw:\n",
    "            return None\n",
    "        if isinstance(raw, str):\n",
    "            raw = raw.encode(\"latin1\")\n",
    "        lines = raw.count(b\"\\n\")\n",
    "        return int((file_size / len(raw)) * lines) if lines > 0 else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# LOAD MANIFEST\n",
    "# ============================================================\n",
    "\n",
    "def read_manifest(path):\n",
    "    raw = dbutils.fs.head(path, 10 * 1024 * 1024)\n",
    "    try:\n",
    "        return json.loads(raw)\n",
    "    except Exception:\n",
    "        rows = spark.read.json(path).collect()\n",
    "        return rows[0] if len(rows) == 1 else {\"parts\": rows}\n",
    "\n",
    "\n",
    "manifest = read_manifest(MANIFEST_PATH)\n",
    "parts = manifest.get(\"parts\", [])\n",
    "\n",
    "if not parts:\n",
    "    raise RuntimeError(\"Manifest contains no parts\")\n",
    "\n",
    "# ============================================================\n",
    "# PROCESS FILES\n",
    "# ============================================================\n",
    "\n",
    "records = []\n",
    "\n",
    "for part in parts:\n",
    "    fname = part.get(\"filename\")\n",
    "    if not fname:\n",
    "        continue\n",
    "\n",
    "    full_path = f\"{RAW_VOLUME_ROOT}/{fname}\"\n",
    "\n",
    "    try:\n",
    "        info = dbutils.fs.ls(full_path)[0]\n",
    "        size = info.size\n",
    "        mod_time = datetime.fromtimestamp(info.modificationTime / 1000)\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "    # checksum\n",
    "    try:\n",
    "        content = spark.read.format(\"binaryFile\").load(full_path).select(\"content\").first()[0]\n",
    "        sha256 = hashlib.sha256(content).hexdigest()\n",
    "        ftype = \"parquet\" if fname.lower().endswith(\".parquet\") else \"csv\"\n",
    "    except Exception:\n",
    "        try:\n",
    "            raw = dbutils.fs.head(full_path, 1024 * 1024)\n",
    "            sha256 = hashlib.sha256(raw.encode(\"latin1\")).hexdigest()\n",
    "            ftype = \"unknown\"\n",
    "        except Exception:\n",
    "            sha256 = None\n",
    "            ftype = \"unknown\"\n",
    "\n",
    "    manifest_count = parse_manifest_row_count(part)\n",
    "    quick_count = None\n",
    "\n",
    "    if fname.lower().endswith(\".csv\"):\n",
    "        if size < QUICK_COUNT_MAX_BYTES:\n",
    "            try:\n",
    "                df_tmp = (\n",
    "                    spark.read\n",
    "                    .option(\"header\", \"true\")\n",
    "                    .option(\"sep\", \";\")\n",
    "                    .option(\"encoding\", \"latin1\")\n",
    "                    .csv(full_path)\n",
    "                )\n",
    "                quick_count = df_tmp.count()\n",
    "            except Exception:\n",
    "                pass\n",
    "        else:\n",
    "            quick_count = estimate_csv_rows(full_path, size)\n",
    "\n",
    "    records.append({\n",
    "        \"filename\": fname,\n",
    "        \"filepath\": full_path,\n",
    "        \"file_size_bytes\": size,\n",
    "        \"modification_time\": mod_time,\n",
    "        \"sha256_checksum\": sha256,\n",
    "        \"manifest_reported_row_count\": manifest_count,\n",
    "        \"quick_count\": quick_count,\n",
    "        \"file_type\": ftype,\n",
    "        \"generation_seed\": manifest.get(\"seed\"),\n",
    "        \"generation_notes\": manifest.get(\"notes\"),\n",
    "        \"ingestion_status\": \"Pending\",\n",
    "        \"ingestion_attempts\": 0,\n",
    "        \"last_ingestion_timestamp\": None,\n",
    "        \"ingestion_batch_id\": INGESTION_BATCH_ID,\n",
    "        \"provenance_json\": json.dumps({\"manifest_entry\": part}),\n",
    "        \"created_at\": datetime.utcnow(),\n",
    "        \"updated_at\": datetime.utcnow(),\n",
    "    })\n",
    "\n",
    "# ============================================================\n",
    "# DATAFRAME CREATION\n",
    "# ============================================================\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"filename\", StringType()),\n",
    "    StructField(\"filepath\", StringType()),\n",
    "    StructField(\"file_size_bytes\", LongType()),\n",
    "    StructField(\"modification_time\", TimestampType()),\n",
    "    StructField(\"sha256_checksum\", StringType()),\n",
    "    StructField(\"manifest_reported_row_count\", LongType()),\n",
    "    StructField(\"quick_count\", LongType()),\n",
    "    StructField(\"file_type\", StringType()),\n",
    "    StructField(\"generation_seed\", StringType()),\n",
    "    StructField(\"generation_notes\", StringType()),\n",
    "    StructField(\"ingestion_status\", StringType()),\n",
    "    StructField(\"ingestion_attempts\", LongType()),\n",
    "    StructField(\"last_ingestion_timestamp\", TimestampType()),\n",
    "    StructField(\"ingestion_batch_id\", StringType()),\n",
    "    StructField(\"provenance_json\", StringType()),\n",
    "    StructField(\"created_at\", TimestampType()),\n",
    "    StructField(\"updated_at\", TimestampType()),\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(records, schema=schema)\n",
    "\n",
    "# ============================================================\n",
    "# MERGE INTO DELTA (UNITY CATALOG SAFE)\n",
    "# ============================================================\n",
    "\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "\n",
    "df.createOrReplaceTempView(\"staging_registry\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "MERGE INTO {FULL_TABLE} AS t\n",
    "USING staging_registry AS s\n",
    "ON t.filename = s.filename\n",
    "WHEN MATCHED THEN UPDATE SET\n",
    "    t.filepath = s.filepath,\n",
    "    t.file_size_bytes = s.file_size_bytes,\n",
    "    t.modification_time = s.modification_time,\n",
    "    t.sha256_checksum = s.sha256_checksum,\n",
    "    t.manifest_reported_row_count = s.manifest_reported_row_count,\n",
    "    t.quick_count = s.quick_count,\n",
    "    t.file_type = s.file_type,\n",
    "    t.generation_seed = s.generation_seed,\n",
    "    t.generation_notes = s.generation_notes,\n",
    "    t.ingestion_status = s.ingestion_status,\n",
    "    t.ingestion_attempts = t.ingestion_attempts,\n",
    "    t.last_ingestion_timestamp = t.last_ingestion_timestamp,\n",
    "    t.ingestion_batch_id = s.ingestion_batch_id,\n",
    "    t.provenance_json = s.provenance_json,\n",
    "    t.updated_at = s.updated_at\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT *\n",
    "\"\"\")\n",
    "\n",
    "# ============================================================\n",
    "# WRITE REGISTRATION REPORT \n",
    "# ============================================================\n",
    "\n",
    "report = {\n",
    "    \"ingestion_batch_id\": INGESTION_BATCH_ID,\n",
    "    \"timestamp_utc\": datetime.utcnow().isoformat(),\n",
    "    \"registered_files\": len(records),\n",
    "    \"files\": [\n",
    "        {\n",
    "            \"filename\": r[\"filename\"],\n",
    "            \"sha256\": r[\"sha256_checksum\"],\n",
    "            \"quick_count\": r[\"quick_count\"],\n",
    "            \"manifest_row_count\": r[\"manifest_reported_row_count\"]\n",
    "        }\n",
    "        for r in records\n",
    "    ]\n",
    "}\n",
    "\n",
    "report_path = f\"{RAW_VOLUME_ROOT}/registration_reports/registration_report_{INGESTION_BATCH_ID}.json\"\n",
    "dbutils.fs.put(report_path, json.dumps(report, indent=2), overwrite=True)\n",
    "\n",
    "print(f\"✔ Registration completed: {len(records)} files\")\n",
    "print(f\"✔ Report written to: {report_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4e4607d-a1ba-4558-bee7-32571670e3b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "register_raw_files_to_registry",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}