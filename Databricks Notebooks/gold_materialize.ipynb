{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e683e73-94f2-4802-b029-bd63adfffbc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spark-67749722-046d-4d62-b414-f0/.ipykernel/2652/command-5795559833217638-91921547:59: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  RUN_TS = datetime.utcnow()\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto-selected ingestion_batch_id = reg-20260103T154251Z\ningestion_batch_id used for this run: reg-20260103T154251Z\nWritten census.gold.dim_age_group (9 rows)\nWritten census.gold.metric_definitions (8 rows)\nPerson rows for Gold processing: 100000\nWritten census.gold.fact_population_by_region_year (9670 rows)\nWritten census.gold.indicators_literacy_employment (360 rows)\nWritten census.gold.fact_household_summary (82642 rows)\nWritten census.gold.income_distribution_by_region_year (360 rows)\nWritten census.gold.small_area_shrinkage_estimates (360 rows)\nWritten census.gold.fact_population_flat_region_year (360 rows)\nWritten census.gold.education_distribution_by_region_year (5387 rows)\nWritten census.gold.education_employment_crosswalk (1440 rows)\nAppended ingestion audit to census.gold.ingestion_audit_v1\nGold materialization complete.\nRun ID: gold_materialize_run_b2e70432\nIngestion batch id: reg-20260103T154251Z\n"
     ]
    }
   ],
   "source": [
    "# Databricks notebook source\n",
    "# Title: gold_materialize_extended.py\n",
    "# Purpose: materialize Gold medallion artifacts, including education tables\n",
    "# Assumptions:\n",
    "#  - Silver conformed tables exist under census.silver (dim_person, dim_household, dim_region, lineage)\n",
    "#  - Unity Catalog is available; write to census.gold\n",
    "#  - Notebook runs with appropriate privileges to create/overwrite gold tables\n",
    "# - This notebook is idempotent: tables are overwritten for the current run\n",
    "\n",
    "from datetime import datetime\n",
    "import json\n",
    "import math\n",
    "import uuid\n",
    "import statistics\n",
    "from typing import List, Tuple\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType, ArrayType, MapType, LongType\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# compute stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "CATALOG = \"census\"\n",
    "GOLD_SCHEMA = \"gold\"\n",
    "GOLD_PREFIX = f\"{CATALOG}.{GOLD_SCHEMA}\"\n",
    "\n",
    "SILVER_PERSON = f\"{CATALOG}.silver.dim_person\"\n",
    "SILVER_HOUSEHOLD = f\"{CATALOG}.silver.dim_household\"\n",
    "SILVER_REGION = f\"{CATALOG}.silver.dim_region\"\n",
    "SILVER_LINEAGE = f\"{CATALOG}.silver.lineage\"\n",
    "\n",
    "# Output Gold table names\n",
    "DIM_AGE = f\"{GOLD_PREFIX}.dim_age_group\"\n",
    "METRIC_DEFS = f\"{GOLD_PREFIX}.metric_definitions\"\n",
    "\n",
    "FACT_POP = f\"{GOLD_PREFIX}.fact_population_by_region_year\"\n",
    "INDICATORS = f\"{GOLD_PREFIX}.indicators_literacy_employment\"\n",
    "FACT_HH = f\"{GOLD_PREFIX}.fact_household_summary\"\n",
    "INCOME_DIST = f\"{GOLD_PREFIX}.income_distribution_by_region_year\"\n",
    "FLAT_FACT = f\"{GOLD_PREFIX}.fact_population_flat_region_year\"\n",
    "SMALL_AREA = f\"{GOLD_PREFIX}.small_area_shrinkage_estimates\"\n",
    "INGEST_AUDIT = f\"{GOLD_PREFIX}.ingestion_audit_v1\"\n",
    "\n",
    "# NEW education tables\n",
    "EDU_DIST = f\"{GOLD_PREFIX}.education_distribution_by_region_year\"\n",
    "EDU_XWALK = f\"{GOLD_PREFIX}.education_employment_crosswalk\"\n",
    "\n",
    "# Partitioning config\n",
    "PARTITION_COL = \"census_year\"\n",
    "\n",
    "# Run metadata\n",
    "RUN_ID = f\"gold_materialize_run_{uuid.uuid4().hex[:8]}\"\n",
    "RUN_TS = datetime.utcnow()\n",
    "\n",
    "# Utility\n",
    "def table_exists(tname: str) -> bool:\n",
    "    try:\n",
    "        return spark.catalog.tableExists(tname)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def ensure_schema():\n",
    "    # Create schema if not exists (Unity Catalog)\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {GOLD_PREFIX}\")\n",
    "\n",
    "ensure_schema()\n",
    "\n",
    "# ---------- pick ingestion_batch_id (widget or auto-detect) ----------\n",
    "def tidy_status_col(col):\n",
    "    return F.lower(F.trim(F.coalesce(F.col(col), F.lit(\"\"))))\n",
    "\n",
    "def pick_ingestion_batch():\n",
    "    # First try widget\n",
    "    try:\n",
    "        widget_val = dbutils.widgets.get(\"ingestion_batch_id\")\n",
    "        if widget_val and widget_val.strip() != \"\":\n",
    "            print(f\"Using supplied ingestion_batch_id widget: {widget_val}\")\n",
    "            return widget_val\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Fallback: inspect bronze file_registry for recent batches with attempts\n",
    "    try:\n",
    "        fr_table = \"census.bronze.file_registry_v1\"\n",
    "        if not spark.catalog.tableExists(fr_table):\n",
    "            print(\"No file_registry found; ingestion_batch_id will be None.\")\n",
    "            return None\n",
    "\n",
    "        fr = spark.table(fr_table).withColumn(\"status_norm\", tidy_status_col(\"ingestion_status\"))\n",
    "        batches = fr.groupBy(\"ingestion_batch_id\").agg(\n",
    "            F.sum(F.coalesce(F.col(\"ingestion_attempts\"), F.lit(0))).alias(\"attempts\"),\n",
    "            F.sum(F.when(F.col(\"status_norm\") == \"succeeded\", 1).otherwise(0)).alias(\"succeeded_count\"),\n",
    "            F.max(\"updated_at\").alias(\"last_updated\")\n",
    "        ).filter(F.col(\"attempts\") > 0)\n",
    "\n",
    "        if batches.limit(1).count() == 0:\n",
    "            print(\"No ingestion attempts found in file_registry; ingestion_batch_id will be None.\")\n",
    "            return None\n",
    "\n",
    "        candidate = batches.orderBy(F.desc(\"succeeded_count\"), F.desc(\"last_updated\")).first()\n",
    "        print(f\"Auto-selected ingestion_batch_id = {candidate['ingestion_batch_id']}\")\n",
    "        return candidate[\"ingestion_batch_id\"]\n",
    "    except Exception as e:\n",
    "        print(\"Error while picking ingestion_batch_id:\", str(e))\n",
    "        return None\n",
    "\n",
    "ingestion_batch_id = pick_ingestion_batch()\n",
    "print(\"ingestion_batch_id used for this run:\", ingestion_batch_id)\n",
    "# RUN_ID will act as the gold run id / run identifier; keep existing name RUN_ID for compatibility\n",
    "\n",
    "# ---------- 1) Create / Overwrite dim_age_group ----------\n",
    "age_bins = [\n",
    "    (\"0-4\", 0, 4),\n",
    "    (\"5-14\", 5, 14),\n",
    "    (\"15-24\", 15, 24),\n",
    "    (\"25-34\", 25, 34),\n",
    "    (\"35-44\", 35, 44),\n",
    "    (\"45-54\", 45, 54),\n",
    "    (\"55-64\", 55, 64),\n",
    "    (\"65-74\", 65, 74),\n",
    "    (\"75+\", 75, 200)\n",
    "]\n",
    "\n",
    "age_rows = [(label, lo, hi) for (label, lo, hi) in age_bins]\n",
    "schema_age = StructType([\n",
    "    StructField(\"age_group\", StringType(), False),\n",
    "    StructField(\"age_min\", IntegerType(), False),\n",
    "    StructField(\"age_max\", IntegerType(), False)\n",
    "])\n",
    "df_age = spark.createDataFrame(age_rows, schema_age)\n",
    "df_age.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"true\").saveAsTable(DIM_AGE)\n",
    "print(f\"Written {DIM_AGE} ({df_age.count()} rows)\")\n",
    "\n",
    "# ---------- 2) Metric definitions (simple examples) ----------\n",
    "metric_defs = [\n",
    "    (\"population_count\", \"Total headcount\", \"COUNT(person_id)\"),\n",
    "    (\"population_by_age_group\", \"Population by age group\", \"SUM(population_count) GROUP BY age_group\"),\n",
    "    (\"literacy_rate\", \"Literacy rate (age 5+)\", \"SUM(literate_count) / SUM(pop_age_5_plus)\"),\n",
    "    (\"employment_rate\", \"Employment rate (age 15+)\", \"SUM(employed_count) / SUM(pop_age_15_plus)\"),\n",
    "    (\"mean_income\", \"Mean annual income (employed)\", \"AVG(annual_income_local)\"),\n",
    "    (\"median_income\", \"Median annual income (employed)\", \"APPROX_PERCENTILE(annual_income_local, 0.5)\"),\n",
    "    (\"gini_income\", \"Gini coefficient of income\", \"Gini(annual_income_local)\"),\n",
    "    (\"dependency_ratio\", \"Dependency ratio\", \"(pop_0_14 + pop_65_plus) / pop_15_64\")\n",
    "]\n",
    "schema_metrics = StructType([\n",
    "    StructField(\"metric_id\", StringType(), False),\n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"sql_definition\", StringType(), True)\n",
    "])\n",
    "df_metrics = spark.createDataFrame(metric_defs, schema_metrics)\n",
    "df_metrics.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"true\").saveAsTable(METRIC_DEFS)\n",
    "print(f\"Written {METRIC_DEFS} ({df_metrics.count()} rows)\")\n",
    "\n",
    "# ---------- 3) Read Silver person (canonical current persons) ----------\n",
    "if not table_exists(SILVER_PERSON):\n",
    "    raise RuntimeError(f\"Required source table not found: {SILVER_PERSON}\")\n",
    "\n",
    "person = spark.table(SILVER_PERSON)\n",
    "# Prefer current persons if SCD2 is used\n",
    "if \"is_current\" in person.columns:\n",
    "    person = person.filter(F.col(\"is_current\") == True)\n",
    "\n",
    "# Ensure essential columns exist; provide aliases/typing\n",
    "person = person.withColumn(\"geoid\", F.col(\"geoid\").cast(\"int\")) \\\n",
    "               .withColumn(\"census_year\", F.col(\"census_year\").cast(\"int\")) \\\n",
    "               .withColumn(\"age\", F.col(\"age\").cast(\"int\")) \\\n",
    "               .withColumn(\"sex\", F.col(\"sex\").cast(\"string\"))\n",
    "\n",
    "# ---------- helper: assign age_group using dim_age_group mapping ----------\n",
    "age_df = df_age  # created above\n",
    "\n",
    "# Broadcast small dim_age for efficient join\n",
    "age_broadcast = F.broadcast(age_df)\n",
    "\n",
    "# Create age_group expression via join: easier to perform a cross join mapping\n",
    "person_with_agegroup = person.join(age_df, (person.age >= age_df.age_min) & (person.age <= age_df.age_max), how=\"left\") \\\n",
    "                             .withColumnRenamed(\"age_group\", \"age_group\") \\\n",
    "                             .select(*person.columns, \"age_group\")\n",
    "\n",
    "# fallback fill\n",
    "person_with_agegroup = person_with_agegroup.withColumn(\"age_group\", F.coalesce(F.col(\"age_group\"), F.lit(\"unknown\")))\n",
    "\n",
    "print(\"Person rows for Gold processing:\", person_with_agegroup.count())\n",
    "\n",
    "# === Add derived flags on the canonical person_with_agegroup DataFrame ===\n",
    "person_with_agegroup = (\n",
    "    person_with_agegroup\n",
    "      .withColumn(\"literacy_flag\",\n",
    "          F.when(\n",
    "              (F.col(\"literacy\") == True) |\n",
    "              (F.lower(F.coalesce(F.col(\"literacy\"), F.lit(\"\"))).isin(\"true\",\"1\",\"yes\")),\n",
    "              F.lit(1)\n",
    "          ).otherwise(F.lit(0))\n",
    "      )\n",
    "      .withColumn(\"is_employed_flag\",\n",
    "          F.when(\n",
    "              F.lower(F.coalesce(F.col(\"employment_status\"), F.lit(\"\"))) == \"employed\",\n",
    "              F.lit(1)\n",
    "          ).otherwise(F.lit(0))\n",
    "      )\n",
    "      .withColumn(\"is_informal_flag\",\n",
    "          F.when(\n",
    "              F.lower(F.coalesce(F.col(\"employment_type\"), F.lit(\"\"))) == \"informal\",\n",
    "              F.lit(1)\n",
    "          ).otherwise(F.lit(0))\n",
    "      )\n",
    ")\n",
    "\n",
    "# ---------- 4) fact_population_by_region_year ----------\n",
    "# Aggregate population_count by geoid, census_year, age_group, sex\n",
    "pop_agg = person_with_agegroup.groupBy(\"geoid\",\"census_year\",\"age_group\",\"sex\") \\\n",
    "                              .agg(F.count(F.lit(1)).cast(\"long\").alias(\"population_count\"))\n",
    "\n",
    "# attach provenance columns\n",
    "pop_agg = pop_agg.withColumn(\"ingestion_batch_id\", F.lit(ingestion_batch_id)) \\\n",
    "                 .withColumn(\"run_id\", F.lit(RUN_ID))\n",
    "\n",
    "# write\n",
    "pop_agg.write.format(\"delta\").mode(\"overwrite\").partitionBy(PARTITION_COL).option(\"overwriteSchema\",\"true\").saveAsTable(FACT_POP)\n",
    "print(f\"Written {FACT_POP} ({pop_agg.count()} rows)\")\n",
    "\n",
    "# ---------- 5) indicators_literacy_employment ----------\n",
    "# Compute key indicators per geoid,census_year\n",
    "p = person_with_agegroup\n",
    "\n",
    "# Normalize literacy/employment booleans and fields defensively\n",
    "# (flags already exist on person_with_agegroup; these lines are harmless redundancy)\n",
    "p = p.withColumn(\"literacy_flag\", F.when(F.col(\"literacy\") == True, 1).otherwise(0))\n",
    "p = p.withColumn(\"is_employed_flag\", F.when(F.col(\"employment_status\") == \"Employed\", 1).otherwise(0))\n",
    "p = p.withColumn(\"is_informal_flag\", F.when(F.col(\"employment_type\") == \"Informal\", 1).otherwise(0))\n",
    "\n",
    "ind = p.groupBy(\"geoid\",\"census_year\").agg(\n",
    "    F.sum(F.when(F.col(\"age\") >= 5, F.col(\"literacy_flag\")).otherwise(0)).alias(\"literate_count\"),\n",
    "    F.sum(F.when(F.col(\"age\") >= 5, 1).otherwise(0)).alias(\"pop_age_5_plus\"),\n",
    "    F.sum(F.when(F.col(\"age\") >= 15, F.col(\"is_employed_flag\")).otherwise(0)).alias(\"employed_count\"),\n",
    "    F.sum(F.when(F.col(\"age\") >= 15, 1).otherwise(0)).alias(\"pop_age_15_plus\"),\n",
    "    F.sum(F.col(\"is_informal_flag\")).alias(\"informal_employed_count\")\n",
    ")\n",
    "\n",
    "# Add derived rates\n",
    "ind = ind.withColumn(\"informal_employment_share\", \n",
    "                     F.when(F.col(\"employed_count\") > 0, F.col(\"informal_employed_count\") / F.col(\"employed_count\")).otherwise(F.lit(None)))\n",
    "\n",
    "# attach provenance columns\n",
    "ind = ind.withColumn(\"ingestion_batch_id\", F.lit(ingestion_batch_id)) \\\n",
    "         .withColumn(\"run_id\", F.lit(RUN_ID))\n",
    "\n",
    "ind = ind.select(\"geoid\",\"census_year\",\"literate_count\",\"pop_age_5_plus\",\"employed_count\",\"pop_age_15_plus\",\"informal_employed_count\",\"informal_employment_share\",\"ingestion_batch_id\",\"run_id\")\n",
    "\n",
    "ind.write.format(\"delta\").mode(\"overwrite\").partitionBy(PARTITION_COL).option(\"overwriteSchema\",\"true\").saveAsTable(INDICATORS)\n",
    "print(f\"Written {INDICATORS} ({ind.count()} rows)\")\n",
    "\n",
    "# ---------- 6) fact_household_summary ----------\n",
    "# build household aggregates from silver.household + person (if available)\n",
    "if table_exists(SILVER_HOUSEHOLD):\n",
    "    hh = spark.table(SILVER_HOUSEHOLD)\n",
    "    # If household table exists, use it as base (could join to persons to enrich)\n",
    "    # But compute summary: household_size, median_household_income, household_literacy_rate\n",
    "    person_income = person_with_agegroup.select(\"household_id\",\"geoid\",\"annual_income_local\",\"literacy_flag\")\n",
    "    # household_size and median income and literacy rate per household\n",
    "    hh_agg = person_income.groupBy(\"household_id\",\"geoid\").agg(\n",
    "        F.count(\"*\").alias(\"household_size\"),\n",
    "        F.expr(\"percentile_approx(annual_income_local, 0.5)\").alias(\"median_household_income\"),\n",
    "        F.avg(F.col(\"literacy_flag\")).alias(\"household_literacy_rate\")\n",
    "    ).withColumn(\"ingestion_batch_id\", F.lit(ingestion_batch_id)).withColumn(\"run_id\", F.lit(RUN_ID))\n",
    "\n",
    "    hh_agg.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"geoid\").option(\"overwriteSchema\",\"true\").saveAsTable(FACT_HH)\n",
    "    print(f\"Written {FACT_HH} ({hh_agg.count()} rows)\")\n",
    "else:\n",
    "    # If no silver household, derive household aggregates directly from persons (best-effort)\n",
    "    hh_agg = person_with_agegroup.groupBy(\"household_id\",\"geoid\").agg(\n",
    "        F.count(\"*\").alias(\"household_size\"),\n",
    "        F.expr(\"percentile_approx(annual_income_local, 0.5)\").alias(\"median_household_income\"),\n",
    "        F.avg(F.col(\"literacy_flag\")).alias(\"household_literacy_rate\")\n",
    "    ).withColumn(\"ingestion_batch_id\", F.lit(ingestion_batch_id)).withColumn(\"run_id\", F.lit(RUN_ID))\n",
    "\n",
    "    hh_agg.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"geoid\").option(\"overwriteSchema\",\"true\").saveAsTable(FACT_HH)\n",
    "    print(f\"Derived & written {FACT_HH} ({hh_agg.count()} rows)\")\n",
    "\n",
    "# ---------- 7) income_distribution_by_region_year  (deciles, mean, median, gini) ----------\n",
    "# Use employed persons with non-null positive incomes\n",
    "employed_income = person_with_agegroup.filter((F.col(\"employment_status\") == \"Employed\") & (F.col(\"annual_income_local\").isNotNull()) & (F.col(\"annual_income_local\") > 0)) \\\n",
    "                                      .select(\"geoid\",\"census_year\",\"annual_income_local\")\n",
    "\n",
    "# Produce grouped collect_list and some aggregates, then compute deciles/gini on driver-side per group\n",
    "grp = employed_income.groupBy(\"geoid\",\"census_year\").agg(\n",
    "    F.collect_list(\"annual_income_local\").alias(\"incomes\"),\n",
    "    F.count(\"*\").alias(\"employed_count\"),\n",
    "    F.avg(\"annual_income_local\").alias(\"mean_income\")\n",
    ")\n",
    "\n",
    "# materialize to pandas for decile/gini calculations (safe for 100k rows)\n",
    "pdf = grp.toPandas()\n",
    "\n",
    "def compute_stats(row):\n",
    "    incomes = row[\"incomes\"] if row[\"incomes\"] is not None else []\n",
    "    n = int(row[\"employed_count\"] or 0)\n",
    "    mean_income = float(row[\"mean_income\"] or 0.0)\n",
    "    if n == 0:\n",
    "        return {\n",
    "            \"geoid\": int(row[\"geoid\"]),\n",
    "            \"census_year\": int(row[\"census_year\"]),\n",
    "            \"employed_count\": 0,\n",
    "            \"mean_income\": None,\n",
    "            \"median_income\": None,\n",
    "            \"deciles\": None,\n",
    "            \"gini_income\": None,\n",
    "            \"topcoded_count\": 0\n",
    "        }\n",
    "    arr = sorted([float(v) for v in incomes])\n",
    "    # median\n",
    "    median = float(statistics.median(arr))\n",
    "    # deciles - 10%,20%...\n",
    "    deciles = [float(np.quantile(arr, q/10.0)) for q in range(1,10)] if len(arr) > 0 else None\n",
    "    # gini calculation\n",
    "    try:\n",
    "        a = np.array(arr)\n",
    "        if a.sum() <= 0 or n == 0:\n",
    "            gini = None\n",
    "        else:\n",
    "            sorted_a = np.sort(a)\n",
    "            index = np.arange(1, n+1)\n",
    "            gini = (2.0 * np.sum(index * sorted_a) - (n + 1) * np.sum(sorted_a)) / (n * np.sum(sorted_a))\n",
    "            gini = float(gini)\n",
    "    except Exception:\n",
    "        gini = None\n",
    "    topcoded = sum(1 for v in arr if v >= 200000)\n",
    "    return {\n",
    "        \"geoid\": int(row[\"geoid\"]),\n",
    "        \"census_year\": int(row[\"census_year\"]),\n",
    "        \"employed_count\": n,\n",
    "        \"mean_income\": mean_income,\n",
    "        \"median_income\": median,\n",
    "        \"deciles\": deciles,\n",
    "        \"gini_income\": gini,\n",
    "        \"topcoded_count\": topcoded\n",
    "    }\n",
    "\n",
    "stats_rows = []\n",
    "for idx, r in pdf.iterrows():\n",
    "    stats_rows.append(compute_stats(r))\n",
    "\n",
    "# build spark dataframe for income dist\n",
    "if len(stats_rows) > 0:\n",
    "    schema_income = StructType([\n",
    "        StructField(\"geoid\", IntegerType(), True),\n",
    "        StructField(\"census_year\", IntegerType(), True),\n",
    "        StructField(\"employed_count\", IntegerType(), True),\n",
    "        StructField(\"mean_income\", DoubleType(), True),\n",
    "        StructField(\"median_income\", DoubleType(), True),\n",
    "        StructField(\"deciles\", ArrayType(DoubleType()), True),\n",
    "        StructField(\"gini_income\", DoubleType(), True),\n",
    "        StructField(\"topcoded_count\", IntegerType(), True)\n",
    "    ])\n",
    "    spark_income = spark.createDataFrame(stats_rows, schema=schema_income)\n",
    "else:\n",
    "    spark_income = spark.createDataFrame([], schema=StructType([\n",
    "        StructField(\"geoid\", IntegerType(), True),\n",
    "        StructField(\"census_year\", IntegerType(), True),\n",
    "        StructField(\"employed_count\", IntegerType(), True),\n",
    "        StructField(\"mean_income\", DoubleType(), True),\n",
    "        StructField(\"median_income\", DoubleType(), True),\n",
    "        StructField(\"deciles\", ArrayType(DoubleType()), True),\n",
    "        StructField(\"gini_income\", DoubleType(), True),\n",
    "        StructField(\"topcoded_count\", IntegerType(), True)\n",
    "    ]))\n",
    "\n",
    "# attach provenance columns\n",
    "spark_income = spark_income.withColumn(\"ingestion_batch_id\", F.lit(ingestion_batch_id)).withColumn(\"run_id\", F.lit(RUN_ID))\n",
    "\n",
    "spark_income.write.format(\"delta\").mode(\"overwrite\").partitionBy(PARTITION_COL).option(\"overwriteSchema\",\"true\").saveAsTable(INCOME_DIST)\n",
    "print(f\"Written {INCOME_DIST} ({spark_income.count()} rows)\")\n",
    "\n",
    "# ---------- 8) small_area_shrinkage_estimates (empirical-Bayes for literacy) ----------\n",
    "# Use indicators table to compute region-level k (literate_count) and n (pop_age_5_plus)\n",
    "ind_df = spark.table(INDICATORS)\n",
    "# compute global prior\n",
    "global_sum = ind_df.agg(F.sum(\"literate_count\").alias(\"K\"), F.sum(\"pop_age_5_plus\").alias(\"N\")).collect()[0]\n",
    "K = global_sum[\"K\"] or 0\n",
    "N = global_sum[\"N\"] or 0\n",
    "global_rate = (K / N) if N and N > 0 else 0.5\n",
    "\n",
    "# choose equivalent sample size m as average sample size across regions (clip to reasonable)\n",
    "avg_n = ind_df.agg(F.avg(\"pop_age_5_plus\").alias(\"avg_n\")).collect()[0][\"avg_n\"] or 10\n",
    "m = max(5, int(avg_n))  # pseudo-counts\n",
    "\n",
    "alpha0 = global_rate * m\n",
    "beta0 = (1.0 - global_rate) * m\n",
    "\n",
    "# compute posterior mean for each region\n",
    "sa = ind_df.withColumn(\"alpha0\", F.lit(float(alpha0))).withColumn(\"beta0\", F.lit(float(beta0))) \\\n",
    "           .withColumn(\"posterior_mean\", (F.col(\"alpha0\") + F.col(\"literate_count\")) / (F.col(\"alpha0\") + F.col(\"beta0\") + F.col(\"pop_age_5_plus\"))) \\\n",
    "           .withColumn(\"k_region\", F.col(\"literate_count\")) \\\n",
    "           .withColumn(\"n_region\", F.col(\"pop_age_5_plus\")) \\\n",
    "           .select(\"geoid\",\"census_year\",\"k_region\",\"n_region\",\"posterior_mean\",\"alpha0\",\"beta0\")\n",
    "\n",
    "sa = sa.withColumnRenamed(\"posterior_mean\", \"shrinkage_literacy_rate\") \\\n",
    "       .withColumnRenamed(\"k_region\", \"k_region\") \\\n",
    "       .withColumnRenamed(\"n_region\", \"n_region\")\n",
    "\n",
    "# attach provenance columns\n",
    "sa = sa.withColumn(\"ingestion_batch_id\", F.lit(ingestion_batch_id)).withColumn(\"run_id\", F.lit(RUN_ID))\n",
    "\n",
    "sa.write.format(\"delta\").mode(\"overwrite\").partitionBy(PARTITION_COL).option(\"overwriteSchema\",\"true\").saveAsTable(SMALL_AREA)\n",
    "print(f\"Written {SMALL_AREA} ({sa.count()} rows)\")\n",
    "\n",
    "# ---------- 9) fact_population_flat_region_year (flatten several components into one table) ----------\n",
    "# Build region-level flat view by joining: region_name from silver.region if available; else use geoid label\n",
    "if table_exists(SILVER_REGION):\n",
    "    region_df = spark.table(SILVER_REGION).select(\"geoid\",\"region_name_standard\",\"iso_admin_code\")\n",
    "    region_df = region_df.withColumnRenamed(\"region_name_standard\",\"region_name\")\n",
    "else:\n",
    "    # derive region name from flattened facts if available (best effort)\n",
    "    region_df = pop_agg.select(\"geoid\").distinct().withColumn(\"region_name\", F.concat(F.lit(\"Region \"), F.col(\"geoid\")))\n",
    "\n",
    "# indicators (ind), income (spark_income), small area (sa), household summary averaged per region-year\n",
    "hh_region = spark.table(FACT_HH).groupBy(\"geoid\",\"ingestion_batch_id\").agg(\n",
    "    F.avg(\"median_household_income\").alias(\"avg_median_hh_income\"),\n",
    "    F.avg(\"household_size\").alias(\"avg_household_size\")\n",
    ")\n",
    "# but hh_region uses ingestion_batch_id rather than census_year; attempt to join via geoid only and later tune\n",
    "# Build a base frame for each geoid,census_year from indicators\n",
    "base = ind_df.alias(\"ind\").join(region_df.alias(\"r\"), on=\"geoid\", how=\"left\") \\\n",
    "                   .select(F.col(\"ind.geoid\"), F.col(\"ind.census_year\"), F.col(\"r.region_name\"), \"literate_count\",\"pop_age_5_plus\",\"employed_count\",\"pop_age_15_plus\",\"informal_employed_count\")\n",
    "\n",
    "# join income stats\n",
    "flat = base.alias(\"b\").join(\n",
    "    spark_income.alias(\"inc\"), \n",
    "    on=[\"geoid\", \"census_year\"], \n",
    "    how=\"left\"\n",
    ").join(\n",
    "    sa.alias(\"s\"), \n",
    "    on=[\"geoid\", \"census_year\"], \n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# join household aggregates by geoid (best-effort)\n",
    "if \"geoid\" in hh_region.columns:\n",
    "    # hh_region uses ingestion_batch_id; aggregate by geoid average\n",
    "    hh_by_geoid = hh_region.groupBy(\"geoid\").agg(F.avg(\"avg_median_hh_income\").alias(\"avg_median_hh_income\"), F.avg(\"avg_household_size\").alias(\"avg_household_size\"))\n",
    "    flat = flat.join(hh_by_geoid.alias(\"h\"), on=\"geoid\", how=\"left\")\n",
    "else:\n",
    "    flat = flat.withColumn(\"avg_median_hh_income\", F.lit(None)).withColumn(\"avg_household_size\", F.lit(None))\n",
    "\n",
    "flat = flat.select(\n",
    "    \"b.*\",                                # Select all from indicators/base\n",
    "    \"inc.mean_income\", \n",
    "    \"inc.median_income\",\n",
    "    \"inc.deciles\",\n",
    "    \"inc.gini_income\",\n",
    "    \"inc.topcoded_count\",\n",
    "    \"s.shrinkage_literacy_rate\",\n",
    "    \"h.avg_median_hh_income\",\n",
    "    \"h.avg_household_size\"\n",
    ")\n",
    "\n",
    "# finalize flattened columns: canonicalize column names used by dashboards\n",
    "flat = flat.withColumnRenamed(\"mean_income\",\"mean_income\") \\\n",
    "           .withColumnRenamed(\"median_income\",\"median_income\") \\\n",
    "           .withColumn(\"pop_age_5_plus_final\", F.col(\"pop_age_5_plus\")) \\\n",
    "           .withColumn(\"pop_age_15_plus\", F.col(\"pop_age_15_plus\")) \\\n",
    "           .withColumn(\"employed_final_count\", F.col(\"employed_count\")) \\\n",
    "           .withColumn(\"literate_count\", F.col(\"literate_count\")) \\\n",
    "           .withColumn(\"gini_income\", F.col(\"gini_income\")) \\\n",
    "           .withColumn(\"topcoded_count\", F.col(\"topcoded_count\")) \\\n",
    "           .withColumn(\"deciles\", F.col(\"deciles\")) \\\n",
    "           .withColumn(\"ingestion_batch_id\", F.lit(ingestion_batch_id)) \\\n",
    "           .withColumn(\"run_id\", F.lit(RUN_ID))\n",
    "\n",
    "# write flat\n",
    "flat.write.format(\"delta\").mode(\"overwrite\").partitionBy(PARTITION_COL).option(\"overwriteSchema\",\"true\").saveAsTable(FLAT_FACT)\n",
    "print(f\"Written {FLAT_FACT} ({flat.count()} rows)\")\n",
    "\n",
    "# ---------- 10) EDUCATION: education_distribution_by_region_year ----------\n",
    "# Build education distribution from silver.dim_person (education_level expected in silver)\n",
    "edu_cols = [\"geoid\",\"census_year\",\"education_level\",\"sex\",\"age\",\"literacy\"]\n",
    "\n",
    "if \"education_level\" not in person_with_agegroup.columns:\n",
    "    # Absent education_level—create an empty table and log\n",
    "    print(\"education_level not found in silver.dim_person; creating empty education tables as placeholders.\")\n",
    "    empty_schema = StructType([\n",
    "        StructField(\"geoid\", IntegerType(), True),\n",
    "        StructField(\"census_year\", IntegerType(), True),\n",
    "        StructField(\"education_level\", StringType(), True),\n",
    "        StructField(\"sex\", StringType(), True),\n",
    "        StructField(\"population_count\", IntegerType(), True),\n",
    "        StructField(\"population_share\", DoubleType(), True),\n",
    "        StructField(\"literate_count\", IntegerType(), True),\n",
    "        StructField(\"literacy_rate_within_level\", DoubleType(), True),\n",
    "        StructField(\"ingestion_batch_id\", StringType(), True),\n",
    "        StructField(\"run_id\", StringType(), True)\n",
    "    ])\n",
    "    spark.createDataFrame([], schema=empty_schema).write.format(\"delta\").mode(\"overwrite\").saveAsTable(EDU_DIST)\n",
    "    spark.createDataFrame([], schema=empty_schema).write.format(\"delta\").mode(\"overwrite\").saveAsTable(EDU_XWALK)\n",
    "else:\n",
    "    # Compute distribution\n",
    "    # population_count by geoid,census_year,education_level,sex\n",
    "    edu_base = person_with_agegroup.filter(F.col(\"education_level\").isNotNull())\n",
    "    edu_agg = edu_base.groupBy(\"geoid\",\"census_year\",\"education_level\",\"sex\").agg(\n",
    "        F.count(\"*\").alias(\"population_count\"),\n",
    "        F.sum(F.when(F.col(\"age\") >= 5, F.col(\"literacy_flag\")).otherwise(0)).alias(\"literate_count\")\n",
    "    )\n",
    "    # compute population_share per geoid,census_year\n",
    "    total_by_region = edu_agg.groupBy(\"geoid\",\"census_year\").agg(F.sum(\"population_count\").alias(\"region_pop\"))\n",
    "    edu = edu_agg.join(total_by_region, on=[\"geoid\",\"census_year\"], how=\"left\") \\\n",
    "                 .withColumn(\"population_share\", F.col(\"population_count\") / F.col(\"region_pop\")) \\\n",
    "                 .withColumn(\"literacy_rate_within_level\", F.when(F.col(\"population_count\") > 0, F.col(\"literate_count\") / F.col(\"population_count\")).otherwise(F.lit(None))) \\\n",
    "                 .select(\"geoid\",\"census_year\",\"education_level\",\"sex\",\"population_count\",\"population_share\",\"literate_count\",\"literacy_rate_within_level\")\n",
    "\n",
    "    # attach provenance columns\n",
    "    edu = edu.withColumn(\"ingestion_batch_id\", F.lit(ingestion_batch_id)).withColumn(\"run_id\", F.lit(RUN_ID))\n",
    "\n",
    "    # write\n",
    "    edu.write.format(\"delta\").mode(\"overwrite\").partitionBy(PARTITION_COL).option(\"overwriteSchema\",\"true\").saveAsTable(EDU_DIST)\n",
    "    print(f\"Written {EDU_DIST} ({edu.count()} rows)\")\n",
    "\n",
    "    # ---------- 11) EDUCATION × EMPLOYMENT CROSSWALK ----------\n",
    "    # For each education_level, geoid,census_year compute employment stats and income stats\n",
    "    # Denominators: persons age >= 15 for employment rate\n",
    "    edu_x = edu_base.filter(F.col(\"age\") >= 15).groupBy(\"geoid\",\"census_year\",\"education_level\").agg(\n",
    "        F.count(\"*\").alias(\"pop_age_15_plus_edu\"),\n",
    "        F.sum(F.when(F.col(\"employment_status\") == \"Employed\", 1).otherwise(0)).alias(\"employed_count_edu\"),\n",
    "        F.sum(F.when(F.col(\"employment_type\") == \"Informal\", 1).otherwise(0)).alias(\"informal_count_edu\")\n",
    "    )\n",
    "\n",
    "    # income stats for employed in that education level\n",
    "    incomes_edu = person_with_agegroup.filter((F.col(\"age\") >= 15) & (F.col(\"employment_status\") == \"Employed\") & (F.col(\"annual_income_local\").isNotNull())).select(\n",
    "        \"geoid\",\"census_year\",\"education_level\",\"annual_income_local\"\n",
    "    )\n",
    "\n",
    "    # compute mean and median per group using collect_list approach and driver-side compute (safe for our size)\n",
    "    inc_grp = incomes_edu.groupBy(\"geoid\",\"census_year\",\"education_level\").agg(\n",
    "        F.collect_list(\"annual_income_local\").alias(\"incomes\"),\n",
    "        F.count(\"*\").alias(\"employed_count_income\")\n",
    "    )\n",
    "\n",
    "    pdf_inc = inc_grp.toPandas()\n",
    "    rows_x = []\n",
    "    for idx, r in pdf_inc.iterrows():\n",
    "        ge = int(r[\"geoid\"])\n",
    "        cy = int(r[\"census_year\"])\n",
    "        edu_lvl = r[\"education_level\"]\n",
    "        incomes = r[\"incomes\"] if r[\"incomes\"] is not None else []\n",
    "        if len(incomes) > 0:\n",
    "            arr = sorted([float(x) for x in incomes])\n",
    "            mean_i = float(sum(arr)/len(arr))\n",
    "            median_i = float(statistics.median(arr))\n",
    "        else:\n",
    "            mean_i = None\n",
    "            median_i = None\n",
    "        rows_x.append({\n",
    "            \"geoid\": ge,\n",
    "            \"census_year\": cy,\n",
    "            \"education_level\": edu_lvl,\n",
    "            \"mean_income\": mean_i,\n",
    "            \"median_income\": median_i\n",
    "        })\n",
    "\n",
    "    # create spark df for income summary\n",
    "    if rows_x:\n",
    "        schema_x = StructType([\n",
    "            StructField(\"geoid\", IntegerType(), True),\n",
    "            StructField(\"census_year\", IntegerType(), True),\n",
    "            StructField(\"education_level\", StringType(), True),\n",
    "            StructField(\"mean_income\", DoubleType(), True),\n",
    "            StructField(\"median_income\", DoubleType(), True)\n",
    "        ])\n",
    "        spark_inc_x = spark.createDataFrame(rows_x, schema=schema_x)\n",
    "    else:\n",
    "        spark_inc_x = spark.createDataFrame([], schema=StructType([\n",
    "            StructField(\"geoid\", IntegerType(), True),\n",
    "            StructField(\"census_year\", IntegerType(), True),\n",
    "            StructField(\"education_level\", StringType(), True),\n",
    "            StructField(\"mean_income\", DoubleType(), True),\n",
    "            StructField(\"median_income\", DoubleType(), True)\n",
    "        ]))\n",
    "\n",
    "    # join the edu_x and income summary\n",
    "    edu_x_full = edu_x.join(spark_inc_x, on=[\"geoid\",\"census_year\",\"education_level\"], how=\"left\") \\\n",
    "                     .withColumn(\"employment_rate\", F.when(F.col(\"pop_age_15_plus_edu\") > 0, F.col(\"employed_count_edu\") / F.col(\"pop_age_15_plus_edu\")).otherwise(F.lit(None))) \\\n",
    "                     .withColumn(\"informal_employment_share\", F.when(F.col(\"employed_count_edu\") > 0, F.col(\"informal_count_edu\") / F.col(\"employed_count_edu\")).otherwise(F.lit(None)))\n",
    "\n",
    "    # attach provenance columns\n",
    "    edu_x_full = edu_x_full.withColumn(\"ingestion_batch_id\", F.lit(ingestion_batch_id)).withColumn(\"run_id\", F.lit(RUN_ID))\n",
    "\n",
    "    edu_x_full = edu_x_full.select(\"geoid\",\"census_year\",\"education_level\",\"pop_age_15_plus_edu\",\"employed_count_edu\",\"employment_rate\",\"informal_employment_share\",\"mean_income\",\"median_income\",\"ingestion_batch_id\",\"run_id\")\n",
    "\n",
    "    edu_x_full.write.format(\"delta\").mode(\"overwrite\").partitionBy(PARTITION_COL).option(\"overwriteSchema\",\"true\").saveAsTable(EDU_XWALK)\n",
    "    print(f\"Written {EDU_XWALK} ({edu_x_full.count()} rows)\")\n",
    "\n",
    "# ---------- 12) ingestion_audit_v1 (append run metadata) ----------\n",
    "# Build a small audit row\n",
    "audit_row = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"ingestion_batch_id\": ingestion_batch_id,\n",
    "    \"run_ts\": RUN_TS.isoformat(),\n",
    "    \"notebook\": \"gold_materialize_extended\",\n",
    "    \"status\": \"SUCCEEDED\",\n",
    "    \"notes\": json.dumps({\n",
    "        \"tables_written\": [\n",
    "            DIM_AGE, METRIC_DEFS, FACT_POP, INDICATORS, FACT_HH, INCOME_DIST, SMALL_AREA, FLAT_FACT, EDU_DIST, EDU_XWALK\n",
    "        ],\n",
    "        \"row_counts\": {\n",
    "            \"fact_population_by_region_year\": int(pop_agg.count()),\n",
    "            \"indicators_literacy_employment\": int(ind.count()),\n",
    "            \"fact_household_summary\": int(hh_agg.count()) if 'hh_agg' in locals() else 0,\n",
    "            \"income_distribution_by_region_year\": int(spark_income.count()),\n",
    "            \"small_area_shrinkage_estimates\": int(sa.count()),\n",
    "            \"fact_population_flat_region_year\": int(flat.count()),\n",
    "            \"education_distribution_by_region_year\": int(spark.table(EDU_DIST).count()) if table_exists(EDU_DIST) else 0,\n",
    "            \"education_employment_crosswalk\": int(spark.table(EDU_XWALK).count()) if table_exists(EDU_XWALK) else 0\n",
    "        }\n",
    "    })\n",
    "}\n",
    "\n",
    "audit_schema = StructType([\n",
    "    StructField(\"run_id\", StringType(), False),\n",
    "    StructField(\"ingestion_batch_id\", StringType(), True),\n",
    "    StructField(\"run_ts\", StringType(), False),\n",
    "    StructField(\"notebook\", StringType(), False),\n",
    "    StructField(\"status\", StringType(), False),\n",
    "    StructField(\"notes\", StringType(), True)\n",
    "])\n",
    "audit_df = spark.createDataFrame([ (audit_row[\"run_id\"], audit_row[\"ingestion_batch_id\"], audit_row[\"run_ts\"], audit_row[\"notebook\"], audit_row[\"status\"], audit_row[\"notes\"]) ], schema=audit_schema)\n",
    "\n",
    "# append or create\n",
    "if table_exists(INGEST_AUDIT):\n",
    "    audit_df.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(INGEST_AUDIT)\n",
    "else:\n",
    "    audit_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(INGEST_AUDIT)\n",
    "\n",
    "print(f\"Appended ingestion audit to {INGEST_AUDIT}\")\n",
    "\n",
    "# ---------- Done ----------\n",
    "print(\"Gold materialization complete.\")\n",
    "print(\"Run ID:\", RUN_ID)\n",
    "print(\"Ingestion batch id:\", ingestion_batch_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dabfc006-759d-43b4-b279-82602bd8141e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "gold_materialize",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}