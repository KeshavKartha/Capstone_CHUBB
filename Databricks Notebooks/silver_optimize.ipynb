{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10630f53-7ff2-4225-89eb-d70657eac869",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook: silver_optimize\n",
    "# Path: /Workspace/Users/you/silver_optimize\n",
    "#\n",
    "# Widgets:\n",
    "#  - ingestion_batch_id (optional)  -> if provided, optimize will try to limit to that partition when possible\n",
    "#\n",
    "# Purpose:\n",
    "#  - Run Delta OPTIMIZE to compact small files into larger files (target ~256MB)\n",
    "#  - Apply Z-ORDER on sensible non-partition columns to improve read locality for common query patterns\n",
    "#  - Be idempotent and safe to run repeatedly; support per-batch (ingestion) scoping\n",
    "\n",
    "from datetime import datetime\n",
    "import json\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "# Tables expected to optimize in the Silver layer\n",
    "tables_to_optimize = [\n",
    "    \"census.silver.dim_person\",\n",
    "    \"census.silver.dim_person_history\",\n",
    "    \"census.silver.dim_household\",\n",
    "    \"census.silver.lineage\"\n",
    "]\n",
    "\n",
    "# Heuristic mapping: for each table choose a non-partition column that is useful for filtering/joining.\n",
    "# These should NOT be partition columns. If the chosen column is not present, the code will fall back.\n",
    "recommended_zorder = {\n",
    "    \"census.silver.dim_person\": [\"canonical_person_id\", \"person_surrogate_id\", \"last_updated\"],\n",
    "    \"census.silver.dim_person_history\": [\"canonical_person_id\", \"person_surrogate_id\", \"census_year\"],\n",
    "    \"census.silver.dim_household\": [\"household_id\", \"median_household_income\", \"geoid\"],\n",
    "    \"census.silver.lineage\": [\"canonical_person_id\", \"ingestion_batch_id\"]\n",
    "}\n",
    "\n",
    "# optional widget\n",
    "try:\n",
    "    ingestion_batch_id = dbutils.widgets.get(\"ingestion_batch_id\")\n",
    "    if ingestion_batch_id and ingestion_batch_id.strip() == \"\":\n",
    "        ingestion_batch_id = None\n",
    "except Exception:\n",
    "    ingestion_batch_id = None\n",
    "\n",
    "start_ts = datetime.utcnow()\n",
    "results = []\n",
    "\n",
    "def table_exists(tbl):\n",
    "    try:\n",
    "        return spark.catalog.tableExists(tbl)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def choose_zorder_column(tbl):\n",
    "    # Return the first recommended column that actually exists in the table schema and that is not a partition column.\n",
    "    cols = [c.name for c in spark.catalog.listColumns(tbl)]\n",
    "    # Try recommended columns in order\n",
    "    for cand in recommended_zorder.get(tbl, []):\n",
    "        if cand in cols:\n",
    "            # Should avoid z-ordering on partition columns; try to detect partition flag if available\n",
    "            try:\n",
    "                # spark.catalog.listColumns returns objects with isPartition attr in many Spark versions\n",
    "                part_cols = [c.name for c in spark.catalog.listColumns(tbl) if getattr(c, \"isPartition\", False)]\n",
    "            except Exception:\n",
    "                # If metadata missing, assume the common partition column names (safe fallback)\n",
    "                part_cols = [\"geoid\", \"ingestion_batch_id\", \"_ingestion_batch_id\", \"partition\"]\n",
    "            if cand in part_cols:\n",
    "                # candidate is a partition column -> skip it\n",
    "                continue\n",
    "            return cand\n",
    "    # if none found, return None\n",
    "    return None\n",
    "\n",
    "for tbl in tables_to_optimize:\n",
    "    if not table_exists(tbl):\n",
    "        results.append({\"table\": tbl, \"status\": \"SKIP\", \"reason\": \"not_exists\"})\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Decide whether I can restrict optimize to an ingestion partition\n",
    "        where_clause = \"\"\n",
    "        # lineage and household tables often have ingestion_batch_id column; check presence and use it if provided\n",
    "        if ingestion_batch_id:\n",
    "            cols_lower = [c.name.lower() for c in spark.catalog.listColumns(tbl)]\n",
    "            if \"ingestion_batch_id\" in cols_lower or \"_ingestion_batch_id\" in cols_lower:\n",
    "                # prefer exact column name on table; choose actual casing\n",
    "                colnames = [c.name for c in spark.catalog.listColumns(tbl)]\n",
    "                chosen_col = next((c for c in colnames if c.lower() in (\"ingestion_batch_id\",\"_ingestion_batch_id\")), None)\n",
    "                if chosen_col:\n",
    "                    where_clause = f\" WHERE {chosen_col} = '{ingestion_batch_id}'\"\n",
    "\n",
    "        # choose z-order candidate for this table\n",
    "        zcol = choose_zorder_column(tbl)\n",
    "\n",
    "        # Build SQL\n",
    "        if zcol:\n",
    "            # safe: do OPTIMIZE ... ZORDER BY (zcol)\n",
    "            sql_opt = f\"OPTIMIZE {tbl}{where_clause} ZORDER BY ({zcol})\"\n",
    "        else:\n",
    "            # no suitable z-order candidate found; do a plain OPTIMIZE (optionally constrained by where_clause)\n",
    "            if where_clause:\n",
    "                # OPTIMIZE with WHERE but no ZORDER\n",
    "                sql_opt = f\"OPTIMIZE {tbl}{where_clause}\"\n",
    "            else:\n",
    "                sql_opt = f\"OPTIMIZE {tbl}\"\n",
    "\n",
    "        # log and execute\n",
    "        print(f\"Running OPTIMIZE for table {tbl}; SQL -> {sql_opt}\")\n",
    "        spark.sql(sql_opt)\n",
    "        results.append({\"table\": tbl, \"status\": \"OK\", \"zorder\": zcol, \"where\": (ingestion_batch_id if where_clause else None)})\n",
    "    except Exception as e:\n",
    "        # If OPTIMIZE fails (e.g., runtime lacks OPTIMIZE privilege), capture the error\n",
    "        results.append({\"table\": tbl, \"status\": \"ERROR\", \"error\": str(e)})\n",
    "\n",
    "end_ts = datetime.utcnow()\n",
    "out = {\"status\":\"DONE\",\"start\": start_ts.isoformat(), \"end\": end_ts.isoformat(), \"results\": results}\n",
    "print(json.dumps(out, indent=2))\n",
    "dbutils.notebook.exit(json.dumps(out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a734c804-e821-435c-9ceb-b9e3df75fd12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_optimize",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}