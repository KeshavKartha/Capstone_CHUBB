{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d1be004-3cdc-4253-9b14-10e905163853",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook: gold_validation\n",
    "# Path: /Workspace/Users/you/gold_validation\n",
    "# Widget: ingestion_batch_id (optional)\n",
    "#\n",
    "# Purpose: Validate Gold layer tables created by gold_materialize\n",
    "# Returns structured JSON via dbutils.notebook.exit()\n",
    "#\n",
    "# Writes:\n",
    "#   - census.gold.validation_reports_v1  (append)\n",
    "#   - census.gold.ingestion_audit_v1     (append)  # Same as gold_materialize uses\n",
    "#\n",
    "# Behavior:\n",
    "#   - If widget ingestion_batch_id provided -> validate that batch\n",
    "#   - If not provided -> auto-select most-recent batch from gold audit table\n",
    "#   - Always exits with structured JSON:\n",
    "#       {\"status\":\"VALIDATION_COMPLETE\", \"validated\": bool, \"report\": {...}}\n",
    "#\n",
    "from datetime import datetime\n",
    "import json\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "GOLD_PREFIX = \"census.gold\"\n",
    "GOLD_AUDIT_TABLE = f\"{GOLD_PREFIX}.ingestion_audit_v1\"\n",
    "\n",
    "# Gold tables to validate\n",
    "GOLD_TABLES = {\n",
    "    \"dim_age_group\": f\"{GOLD_PREFIX}.dim_age_group\",\n",
    "    \"metric_definitions\": f\"{GOLD_PREFIX}.metric_definitions\",\n",
    "    \"fact_population_by_region_year\": f\"{GOLD_PREFIX}.fact_population_by_region_year\",\n",
    "    \"indicators_literacy_employment\": f\"{GOLD_PREFIX}.indicators_literacy_employment\",\n",
    "    \"fact_household_summary\": f\"{GOLD_PREFIX}.fact_household_summary\",\n",
    "    \"income_distribution_by_region_year\": f\"{GOLD_PREFIX}.income_distribution_by_region_year\",\n",
    "    \"small_area_shrinkage_estimates\": f\"{GOLD_PREFIX}.small_area_shrinkage_estimates\",\n",
    "    \"fact_population_flat_region_year\": f\"{GOLD_PREFIX}.fact_population_flat_region_year\",\n",
    "    \"education_distribution_by_region_year\": f\"{GOLD_PREFIX}.education_distribution_by_region_year\",\n",
    "    \"education_employment_crosswalk\": f\"{GOLD_PREFIX}.education_employment_crosswalk\"\n",
    "}\n",
    "\n",
    "VALIDATION_TABLE = f\"{GOLD_PREFIX}.validation_reports_v1\"\n",
    "\n",
    "# thresholds (tunable)\n",
    "MIN_FACT_ROWS_THRESHOLD = 1  # At least 1 row in fact tables\n",
    "EXPECTED_AGE_GROUPS = 9  # dim_age_group should have 9 rows\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def now():\n",
    "    return datetime.utcnow()\n",
    "\n",
    "def table_exists(tname: str) -> bool:\n",
    "    try:\n",
    "        return spark.catalog.tableExists(tname)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def pick_ingestion_batch():\n",
    "    \"\"\"\n",
    "    Select the ingestion_batch_id to validate:\n",
    "      - prefer explicit widget\n",
    "      - otherwise choose the most recent batch from gold audit table\n",
    "    \"\"\"\n",
    "    try:\n",
    "        widget_val = dbutils.widgets.get(\"ingestion_batch_id\")\n",
    "        if widget_val and widget_val.strip() != \"\":\n",
    "            print(f\"Using widget ingestion_batch_id: {widget_val}\")\n",
    "            return widget_val\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Try to get from gold audit table\n",
    "    if table_exists(GOLD_AUDIT_TABLE):\n",
    "        try:\n",
    "            # Get the most recent run for any batch\n",
    "            latest_run = spark.table(GOLD_AUDIT_TABLE) \\\n",
    "                .filter(F.col(\"status\") == \"SUCCEEDED\") \\\n",
    "                .orderBy(F.desc(\"run_ts\")) \\\n",
    "                .select(\"ingestion_batch_id\") \\\n",
    "                .first()\n",
    "            \n",
    "            if latest_run:\n",
    "                batch_id = latest_run[\"ingestion_batch_id\"]\n",
    "                print(f\"Auto-selected ingestion_batch_id from gold audit: {batch_id}\")\n",
    "                return batch_id\n",
    "        except Exception as e:\n",
    "            print(f\"Error selecting batch from gold audit: {str(e)}\")\n",
    "    \n",
    "    # Fallback: try to get from silver validation (upstream)\n",
    "    try:\n",
    "        silver_val_table = \"census.silver.validation_reports_v1\"\n",
    "        if table_exists(silver_val_table):\n",
    "            latest_silver = spark.table(silver_val_table) \\\n",
    "                .filter(F.col(\"status\") == \"PASS\") \\\n",
    "                .orderBy(F.desc(\"report_time\")) \\\n",
    "                .select(\"ingestion_batch_id\") \\\n",
    "                .first()\n",
    "            \n",
    "            if latest_silver:\n",
    "                batch_id = latest_silver[\"ingestion_batch_id\"]\n",
    "                print(f\"Auto-selected ingestion_batch_id from silver validation: {batch_id}\")\n",
    "                return batch_id\n",
    "    except Exception as e:\n",
    "        print(f\"Error selecting batch from silver validation: {str(e)}\")\n",
    "    \n",
    "    # If still nothing, exit with error\n",
    "    dbutils.notebook.exit(json.dumps({\n",
    "        \"status\": \"VALIDATION_COMPLETE\",\n",
    "        \"validated\": False,\n",
    "        \"report\": {\n",
    "            \"reason\": \"no_ingestion_batch_id_found\",\n",
    "            \"message\": \"Provide ingestion_batch_id widget or ensure gold audit table exists with successful runs\"\n",
    "        }\n",
    "    }))\n",
    "\n",
    "# ---------- start ----------\n",
    "ingestion_batch_id = pick_ingestion_batch()\n",
    "start_ts = now()\n",
    "run_id = f\"gold_validation-{ingestion_batch_id}-{start_ts.strftime('%Y%m%dT%H%M%SZ')}\"\n",
    "\n",
    "errors = []\n",
    "warnings = []\n",
    "notes = {}\n",
    "table_counts = {}\n",
    "\n",
    "print(f\"Starting gold validation for batch: {ingestion_batch_id}\")\n",
    "print(f\"Run ID: {run_id}\")\n",
    "\n",
    "# ---------- 1) Check all gold tables exist ----------\n",
    "missing_tables = []\n",
    "for table_name, table_path in GOLD_TABLES.items():\n",
    "    if not table_exists(table_path):\n",
    "        missing_tables.append(table_name)\n",
    "        errors.append(f\"missing_table:{table_name}\")\n",
    "    else:\n",
    "        notes[f\"{table_name}_exists\"] = True\n",
    "\n",
    "if missing_tables:\n",
    "    print(f\"ERROR: Missing gold tables: {missing_tables}\")\n",
    "else:\n",
    "    print(\"✓ All expected gold tables exist\")\n",
    "\n",
    "# ---------- 2) Validate each table ----------\n",
    "if not missing_tables:\n",
    "    for table_name, table_path in GOLD_TABLES.items():\n",
    "        try:\n",
    "            df = spark.table(table_path)\n",
    "            row_count = df.count()\n",
    "            table_counts[table_name] = row_count\n",
    "            notes[f\"{table_name}_count\"] = row_count\n",
    "            \n",
    "            # Table-specific validations\n",
    "            if row_count == 0:\n",
    "                if \"fact\" in table_name or \"indicators\" in table_name:\n",
    "                    errors.append(f\"empty_fact_table:{table_name}\")\n",
    "                    print(f\"ERROR: {table_name} is empty (0 rows)\")\n",
    "                else:\n",
    "                    warnings.append(f\"empty_dimension_table:{table_name}\")\n",
    "                    print(f\"WARNING: {table_name} is empty (0 rows)\")\n",
    "            \n",
    "            # Check for ingestion_batch_id in fact tables (if column exists)\n",
    "            if \"ingestion_batch_id\" in df.columns and row_count > 0:\n",
    "                batch_rows = df.filter(F.col(\"ingestion_batch_id\") == ingestion_batch_id).count()\n",
    "                notes[f\"{table_name}_batch_rows\"] = batch_rows\n",
    "                \n",
    "                if batch_rows == 0 and (\"fact\" in table_name or \"indicators\" in table_name):\n",
    "                    warnings.append(f\"no_rows_for_batch:{table_name}\")\n",
    "                    print(f\"WARNING: {table_name} has no rows for batch {ingestion_batch_id}\")\n",
    "            \n",
    "            # Specific checks for dimension tables\n",
    "            if table_name == \"dim_age_group\":\n",
    "                if row_count != EXPECTED_AGE_GROUPS:\n",
    "                    errors.append(f\"dim_age_group_wrong_row_count: expected={EXPECTED_AGE_GROUPS}, actual={row_count}\")\n",
    "                    print(f\"ERROR: dim_age_group has {row_count} rows, expected {EXPECTED_AGE_GROUPS}\")\n",
    "            \n",
    "            # Check for nulls in key columns (example)\n",
    "            if table_name == \"fact_population_by_region_year\" and row_count > 0:\n",
    "                null_geoid = df.filter(F.col(\"geoid\").isNull()).count()\n",
    "                null_census_year = df.filter(F.col(\"census_year\").isNull()).count()\n",
    "                \n",
    "                if null_geoid > 0:\n",
    "                    errors.append(f\"null_geoid_in_{table_name}:count={null_geoid}\")\n",
    "                if null_census_year > 0:\n",
    "                    errors.append(f\"null_census_year_in_{table_name}:count={null_census_year}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            errors.append(f\"table_check_error:{table_name}:{str(e)}\")\n",
    "            print(f\"ERROR checking {table_name}: {str(e)}\")\n",
    "\n",
    "# ---------- 3) Cross-table consistency checks ----------\n",
    "try:\n",
    "    # Check that fact_population_by_region_year has at least as many regions as indicators\n",
    "    if \"fact_population_by_region_year\" in table_counts and \"indicators_literacy_employment\" in table_counts:\n",
    "        fact_regions = spark.table(GOLD_TABLES[\"fact_population_by_region_year\"]).select(\"geoid\", \"census_year\").distinct().count()\n",
    "        ind_regions = spark.table(GOLD_TABLES[\"indicators_literacy_employment\"]).select(\"geoid\", \"census_year\").distinct().count()\n",
    "        \n",
    "        notes[\"fact_unique_regions\"] = fact_regions\n",
    "        notes[\"indicators_unique_regions\"] = ind_regions\n",
    "        \n",
    "        if fact_regions < ind_regions:\n",
    "            warnings.append(\"indicators_has_more_regions_than_fact_pop\")\n",
    "            print(f\"WARNING: indicators has {ind_regions} unique regions, fact_population has {fact_regions}\")\n",
    "except Exception as e:\n",
    "    warnings.append(f\"cross_table_check_error:{str(e)}\")\n",
    "\n",
    "# ---------- 4) Check gold audit table has entry for this batch ----------\n",
    "try:\n",
    "    if table_exists(GOLD_AUDIT_TABLE):\n",
    "        audit_rows = spark.table(GOLD_AUDIT_TABLE) \\\n",
    "            .filter(F.col(\"ingestion_batch_id\") == ingestion_batch_id) \\\n",
    "            .filter(F.col(\"status\") == \"SUCCEEDED\") \\\n",
    "            .count()\n",
    "        \n",
    "        notes[\"gold_audit_rows_for_batch\"] = audit_rows\n",
    "        \n",
    "        if audit_rows == 0:\n",
    "            warnings.append(\"no_gold_audit_entry_for_batch\")\n",
    "            print(f\"WARNING: No SUCCEEDED entry in gold audit table for batch {ingestion_batch_id}\")\n",
    "    else:\n",
    "        warnings.append(\"gold_audit_table_missing\")\n",
    "except Exception as e:\n",
    "    warnings.append(f\"gold_audit_check_error:{str(e)}\")\n",
    "\n",
    "# ---------- 5) Summary statistics ----------\n",
    "total_gold_rows = sum(table_counts.values()) if table_counts else 0\n",
    "notes[\"total_gold_rows\"] = total_gold_rows\n",
    "notes[\"table_counts\"] = table_counts\n",
    "\n",
    "print(f\"\\n=== GOLD VALIDATION SUMMARY ===\")\n",
    "print(f\"Total gold rows across all tables: {total_gold_rows}\")\n",
    "print(f\"Errors: {len(errors)}\")\n",
    "print(f\"Warnings: {len(warnings)}\")\n",
    "for table, count in table_counts.items():\n",
    "    print(f\"  {table}: {count} rows\")\n",
    "\n",
    "# ---------- Assemble report ----------\n",
    "report = {\n",
    "    \"ingestion_batch_id\": ingestion_batch_id,\n",
    "    \"run_id\": run_id,\n",
    "    \"start_time\": start_ts.isoformat(),\n",
    "    \"end_time\": now().isoformat(),\n",
    "    \"errors\": errors,\n",
    "    \"warnings\": warnings,\n",
    "    \"notes\": notes,\n",
    "    \"table_counts\": table_counts,\n",
    "    \"total_gold_rows\": total_gold_rows,\n",
    "    \"validation_layer\": \"gold\"\n",
    "}\n",
    "\n",
    "# ---------- Write to validation table ----------\n",
    "status_flag = \"PASS\" if not errors else \"ERROR\"\n",
    "\n",
    "try:\n",
    "    # Create validation table if it doesn't exist\n",
    "    if not table_exists(VALIDATION_TABLE):\n",
    "        spark.sql(f\"\"\"\n",
    "            CREATE TABLE {VALIDATION_TABLE} (\n",
    "                ingestion_batch_id STRING,\n",
    "                run_id STRING,\n",
    "                report_time TIMESTAMP,\n",
    "                report_json STRING,\n",
    "                status STRING\n",
    "            )\n",
    "            USING DELTA\n",
    "        \"\"\")\n",
    "        print(f\"Created validation table: {VALIDATION_TABLE}\")\n",
    "    \n",
    "    # Write validation result\n",
    "    spark.createDataFrame([(ingestion_batch_id, run_id, now(), json.dumps(report), status_flag)],\n",
    "                         schema=\"ingestion_batch_id string, run_id string, report_time timestamp, report_json string, status string\") \\\n",
    "         .write.format(\"delta\").mode(\"append\").saveAsTable(VALIDATION_TABLE)\n",
    "    print(f\"✓ Wrote validation result to {VALIDATION_TABLE}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    report[\"validation_table_write_error\"] = str(e)\n",
    "    print(f\"ERROR writing to validation table: {str(e)}\")\n",
    "\n",
    "# ---------- Write to ingestion audit table ----------\n",
    "try:\n",
    "    if table_exists(GOLD_AUDIT_TABLE):\n",
    "        audit_notes = json.dumps({\n",
    "            \"validation_summary\": {\n",
    "                \"errors\": len(errors),\n",
    "                \"warnings\": len(warnings),\n",
    "                \"validated\": len(errors) == 0\n",
    "            },\n",
    "            \"note\": \"gold_validation run\"\n",
    "        })\n",
    "        \n",
    "        audit_row = (run_id, ingestion_batch_id, start_ts.isoformat(), now().isoformat(), \n",
    "                    \"VALIDATION_SUCCEEDED\" if not errors else \"VALIDATION_FAILED\", \n",
    "                    audit_notes)\n",
    "        \n",
    "        audit_df = spark.createDataFrame([audit_row], \n",
    "                                        schema=\"run_id string, ingestion_batch_id string, start_time string, end_time string, status string, notes string\")\n",
    "        \n",
    "        audit_df.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(GOLD_AUDIT_TABLE)\n",
    "        print(f\"✓ Wrote audit entry to {GOLD_AUDIT_TABLE}\")\n",
    "    else:\n",
    "        warnings.append(\"gold_audit_table_missing_could_not_write\")\n",
    "except Exception as e:\n",
    "    report[\"audit_table_write_error\"] = str(e)\n",
    "    print(f\"ERROR writing to audit table: {str(e)}\")\n",
    "\n",
    "# ---------- Final result ----------\n",
    "validated_bool = (len(errors) == 0)\n",
    "result = {\n",
    "    \"status\": \"VALIDATION_COMPLETE\",\n",
    "    \"validated\": validated_bool,\n",
    "    \"report\": report\n",
    "}\n",
    "\n",
    "print(f\"\\n=== FINAL RESULT ===\")\n",
    "print(f\"Validation passed: {validated_bool}\")\n",
    "print(f\"Returning structured JSON result\")\n",
    "\n",
    "# Return structured JSON for Airflow\n",
    "dbutils.notebook.exit(json.dumps(result))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "gold_validation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}