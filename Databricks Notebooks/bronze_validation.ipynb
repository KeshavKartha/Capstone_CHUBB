{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f084b11-a936-4200-981b-f83d59c5f256",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook: bronze_validation\n",
    "# Path: /Workspace/Users/you/bronze_validation\n",
    "# Widget: optional ingestion_batch_id (string)\n",
    "#\n",
    "# Writes:\n",
    "#  - census.bronze.validation_reports_v1\n",
    "#  - updates census.bronze.file_registry_v1 when appropriate (mark Failed on validation errors)\n",
    "#  - appends run row to census.bronze.ingestion_audit_v1\n",
    "#\n",
    "# Behavior:\n",
    "#  - If widget ingestion_batch_id provided -> validate that batch (fail early if not found)\n",
    "#  - If not provided -> auto-select most-recent batch with ingestion_attempts > 0\n",
    "#  - Always exits with structured JSON: {\"status\":\"VALIDATION_COMPLETE\", \"validated\": bool, \"report\": {...}}\n",
    "#    This allows Airflow to decide on reingest vs continue. Only unexpected exceptions will crash the notebook.\n",
    "\n",
    "from datetime import datetime\n",
    "import json\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "FILE_REG_TABLE = \"census.bronze.file_registry_v1\"\n",
    "BRONZE_TABLE = \"census.bronze.individuals_raw_v1\"\n",
    "VALIDATION_TABLE = \"census.bronze.validation_reports_v1\"\n",
    "INGESTION_AUDIT = \"census.bronze.ingestion_audit_v1\"\n",
    "\n",
    "# thresholds (tunable)\n",
    "DEFAULT_MANIFEST_TOL_PCT = 0.001   # Â±0.1%\n",
    "AGE_HARD_LOW = 0\n",
    "AGE_HARD_HIGH = 120\n",
    "AGE_HARD_PASS_RATE = 0.999         # 99.9% must be within bounds\n",
    "REQUIRED_COL_NULL_TOL = 0.0001     # 0.01%\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def now():\n",
    "    return datetime.utcnow()\n",
    "\n",
    "def tidy_status_col(col):\n",
    "    return F.lower(F.trim(F.coalesce(F.col(col), F.lit(\"\"))))\n",
    "\n",
    "def available_batches_summary():\n",
    "    fr = spark.table(FILE_REG_TABLE)\n",
    "    s = fr.withColumn(\"status_norm\", tidy_status_col(\"ingestion_status\"))\n",
    "    agg = s.groupBy(\"ingestion_batch_id\").agg(\n",
    "        F.count(F.when(F.col(\"status_norm\") == \"pending\", True)).alias(\"pending_count\"),\n",
    "        F.count(F.when(F.col(\"status_norm\") == \"processing\", True)).alias(\"processing_count\"),\n",
    "        F.count(F.when(F.col(\"status_norm\") == \"succeeded\", True)).alias(\"succeeded_count\"),\n",
    "        F.count(F.when(F.col(\"status_norm\") == \"failed\", True)).alias(\"failed_count\"),\n",
    "        F.sum(F.coalesce(F.col(\"ingestion_attempts\"), F.lit(0))).alias(\"ingestion_attempts\"),\n",
    "        F.max(\"updated_at\").alias(\"last_updated\"),\n",
    "        F.count(F.lit(1)).alias(\"total_files\")\n",
    "    )\n",
    "    return [r.asDict() for r in agg.orderBy(F.desc(\"ingestion_attempts\"), F.desc(\"succeeded_count\"), F.desc(\"last_updated\")).collect()]\n",
    "\n",
    "# ---------- determine ingestion_batch_id ----------\n",
    "try:\n",
    "    ingestion_batch_id_widget = dbutils.widgets.get(\"ingestion_batch_id\")\n",
    "    if ingestion_batch_id_widget is not None and ingestion_batch_id_widget.strip() == \"\":\n",
    "        ingestion_batch_id_widget = None\n",
    "except Exception:\n",
    "    ingestion_batch_id_widget = None\n",
    "\n",
    "# optional widget to override manifest tolerance\n",
    "try:\n",
    "    manifest_tol = float(dbutils.widgets.get(\"manifest_tolerance_pct\"))\n",
    "except Exception:\n",
    "    manifest_tol = DEFAULT_MANIFEST_TOL_PCT\n",
    "\n",
    "# Quick sanity: ensure file_registry exists and has rows\n",
    "if not spark.catalog.tableExists(FILE_REG_TABLE):\n",
    "    out = {\"status\":\"ERROR\",\"reason\":\"missing_file_registry_table\",\"message\": f\"{FILE_REG_TABLE} does not exist\"}\n",
    "    dbutils.notebook.exit(json.dumps(out))\n",
    "\n",
    "batches_summary = available_batches_summary()\n",
    "\n",
    "if ingestion_batch_id_widget:\n",
    "    # confirm registry rows for requested batch exist\n",
    "    reg_rows = spark.table(FILE_REG_TABLE).filter(F.col(\"ingestion_batch_id\") == ingestion_batch_id_widget).select(\"filename\",\"ingestion_status\",\"ingestion_attempts\").collect()\n",
    "    if not reg_rows:\n",
    "        out = {\"status\":\"ERROR\",\"reason\":\"no_registry_rows_for_requested_batch\",\"requested_batch\": ingestion_batch_id_widget, \"available_batches_summary\": batches_summary}\n",
    "        dbutils.notebook.exit(json.dumps(out))\n",
    "    chosen_batch = ingestion_batch_id_widget\n",
    "else:\n",
    "    # auto-select: choose most recent batch where ingestion_attempts > 0 (evidence that an ingestion was attempted)\n",
    "    fr = spark.table(FILE_REG_TABLE).withColumn(\"status_norm\", tidy_status_col(\"ingestion_status\"))\n",
    "    batches = fr.groupBy(\"ingestion_batch_id\").agg(\n",
    "        F.sum(F.coalesce(F.col(\"ingestion_attempts\"), F.lit(0))).alias(\"attempts\"),\n",
    "        F.sum(F.when(F.col(\"status_norm\") == \"succeeded\", 1).otherwise(0)).alias(\"succeeded_count\"),\n",
    "        F.sum(F.when(F.col(\"status_norm\") == \"failed\", 1).otherwise(0)).alias(\"failed_count\"),\n",
    "        F.max(\"updated_at\").alias(\"last_updated\")\n",
    "    ).filter(F.col(\"attempts\") > 0)\n",
    "\n",
    "    if batches.limit(1).count() == 0:\n",
    "        out = {\"status\":\"NO_BATCH_TO_VALIDATE\",\"message\":\"No batch found with ingestion attempts. Ensure register + ingest have run.\",\"available_batches_summary\": batches_summary}\n",
    "        dbutils.notebook.exit(json.dumps(out))\n",
    "\n",
    "    candidate = batches.orderBy(F.desc(\"succeeded_count\"), F.desc(\"last_updated\")).first()\n",
    "    chosen_batch = candidate[\"ingestion_batch_id\"]\n",
    "\n",
    "print(\"Selected ingestion_batch_id for validation:\", chosen_batch)\n",
    "\n",
    "# ---------- perform validation ----------\n",
    "start_ts = now()\n",
    "run_id = f\"bronze_validation-{chosen_batch}-{start_ts.strftime('%Y%m%dT%H%M%SZ')}\"\n",
    "\n",
    "# fetch registry rows for batch\n",
    "reg_df = spark.table(FILE_REG_TABLE).filter(F.col(\"ingestion_batch_id\") == chosen_batch)\n",
    "reg_rows = reg_df.select(\"filename\",\"manifest_reported_row_count\",\"sha256_checksum\",\"ingestion_status\").collect()\n",
    "if not reg_rows:\n",
    "    out = {\"status\":\"ERROR\",\"reason\":\"no_registry_rows_for_batch_after_selection\",\"selected_batch\": chosen_batch, \"available_batches_summary\": batches_summary}\n",
    "    dbutils.notebook.exit(json.dumps(out))\n",
    "\n",
    "manifest_counts = {r[\"filename\"]: int(r[\"manifest_reported_row_count\"] or 0) for r in reg_rows}\n",
    "manifest_total = sum(manifest_counts.values())\n",
    "\n",
    "# read bronze partition for batch (if table missing, exit with error JSON)\n",
    "if not spark.catalog.tableExists(BRONZE_TABLE):\n",
    "    out = {\"status\":\"ERROR\",\"reason\":\"missing_bronze_table\",\"table\": BRONZE_TABLE}\n",
    "    dbutils.notebook.exit(json.dumps(out))\n",
    "\n",
    "bronze_df = spark.table(BRONZE_TABLE).filter(F.col(\"_ingestion_batch_id\") == chosen_batch)\n",
    "observed_total = bronze_df.count()\n",
    "\n",
    "pct_diff = abs(observed_total - manifest_total) / manifest_total if manifest_total > 0 else 1.0\n",
    "\n",
    "errors = []\n",
    "warnings = []\n",
    "\n",
    "# Hard rule: manifest agreement within tolerance -> considered validation error if exceeded\n",
    "if manifest_total > 0 and pct_diff > manifest_tol:\n",
    "    errors.append({\"code\":\"manifest_count_mismatch\",\"message\":f\"observed {observed_total} vs manifest {manifest_total}\", \"pct_diff\": pct_diff})\n",
    "\n",
    "# Required columns null fraction test\n",
    "required_cols = [\"person_id\",\"geoid\",\"census_year\",\"date_of_birth\"]\n",
    "null_fractions = {}\n",
    "for c in required_cols:\n",
    "    null_count = bronze_df.filter(F.col(c).isNull()).count()\n",
    "    frac = null_count / max(observed_total,1)\n",
    "    null_fractions[c] = frac\n",
    "    if frac > REQUIRED_COL_NULL_TOL:\n",
    "        errors.append({\"code\":\"required_col_null_excess\",\"column\":c,\"null_fraction\":frac})\n",
    "\n",
    "# Age distribution checks\n",
    "age_total = bronze_df.filter(F.col(\"age\").isNotNull()).count()\n",
    "age_invalid = bronze_df.filter((F.col(\"age\").isNotNull()) & ((F.col(\"age\") < AGE_HARD_LOW) | (F.col(\"age\") > AGE_HARD_HIGH))).count()\n",
    "age_invalid_frac = age_invalid / max(max(age_total,1),1)\n",
    "if age_invalid_frac > (1 - AGE_HARD_PASS_RATE):\n",
    "    errors.append({\"code\":\"age_bounds_exceeded\",\"invalid_count\":age_invalid,\"age_count\":age_total,\"invalid_fraction\":age_invalid_frac})\n",
    "\n",
    "# Soft rule: negative income fraction\n",
    "neg_income_count = bronze_df.filter(F.col(\"annual_income_local\").isNotNull() & (F.col(\"annual_income_local\") < 0)).count()\n",
    "neg_income_frac = neg_income_count / max(observed_total,1)\n",
    "if neg_income_frac > 0.001:\n",
    "    warnings.append({\"code\":\"negative_income_fraction_high\",\"fraction\":neg_income_frac})\n",
    "\n",
    "# numeric summaries (safe)\n",
    "def safe_percentiles(df, col):\n",
    "    try:\n",
    "        # returns array of percentiles\n",
    "        p = df.select(F.expr(f\"percentile_approx({col}, array(0.01,0.25,0.5,0.75,0.99)) as pcts\")).first()[0]\n",
    "        return p\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "age_pcts = safe_percentiles(bronze_df, \"age\")\n",
    "income_pcts = safe_percentiles(bronze_df.filter(F.col(\"annual_income_local\").isNotNull()), \"annual_income_local\")\n",
    "\n",
    "# top region variants\n",
    "top_regions = bronze_df.groupBy(\"region_name_reported\").count().orderBy(F.desc(\"count\")).limit(25).collect()\n",
    "top_region_variants = [{\"region_name_reported\": r[\"region_name_reported\"], \"count\": int(r[\"count\"])} for r in top_regions]\n",
    "\n",
    "# assemble report\n",
    "report = {\n",
    "    \"ingestion_batch_id\": chosen_batch,\n",
    "    \"run_id\": run_id,\n",
    "    \"start_time\": start_ts.isoformat(),\n",
    "    \"end_time\": now().isoformat(),\n",
    "    \"manifest_total\": manifest_total,\n",
    "    \"observed_total\": observed_total,\n",
    "    \"pct_diff\": pct_diff,\n",
    "    \"manifest_counts\": manifest_counts,\n",
    "    \"null_fractions\": null_fractions,\n",
    "    \"age_invalid_count\": age_invalid,\n",
    "    \"age_invalid_fraction\": age_invalid_frac,\n",
    "    \"negative_income_count\": neg_income_count,\n",
    "    \"negative_income_fraction\": neg_income_frac,\n",
    "    \"age_percentiles\": age_pcts,\n",
    "    \"income_percentiles\": income_pcts,\n",
    "    \"top_region_variants\": top_region_variants,\n",
    "    \"warnings\": warnings,\n",
    "    \"errors\": errors\n",
    "}\n",
    "\n",
    "# persist to validation_reports table (append)\n",
    "status_flag = \"PASS\" if not errors else \"ERROR\"\n",
    "spark.createDataFrame([(chosen_batch, run_id, now(), json.dumps(report), status_flag)], schema=\"ingestion_batch_id string, run_id string, report_time timestamp, report_json string, status string\").write.format(\"delta\").mode(\"append\").saveAsTable(VALIDATION_TABLE)\n",
    "\n",
    "# If validation errors present, mark registry rows Failed (so operators can inspect). Do NOT raise/exit with error.\n",
    "if errors:\n",
    "    try:\n",
    "        file_registry = DeltaTable.forName(spark, FILE_REG_TABLE)\n",
    "        file_registry.update(\n",
    "            condition = F.expr(f\"ingestion_batch_id = '{chosen_batch}' AND ingestion_status != 'Failed'\"),\n",
    "            set = {\n",
    "                \"ingestion_status\": F.lit(\"Failed\"),\n",
    "                \"updated_at\": F.lit(datetime.utcnow())\n",
    "            }\n",
    "        )\n",
    "    except Exception as e_upd:\n",
    "        # log but continue\n",
    "        print(\"Warning: failed to mark registry rows as Failed:\", str(e_upd))\n",
    "\n",
    "# append run row to ingestion audit (status indicates there were validation errors)\n",
    "ingest_status = \"SUCCEEDED\" if not errors else \"FAILED_VALIDATION\"\n",
    "spark.createDataFrame([(chosen_batch, run_id, start_ts, datetime.utcnow(), ingest_status, json.dumps({\"errors\": errors, \"warnings\": warnings}))], schema=\"ingestion_batch_id string, run_id string, start_time timestamp, end_time timestamp, status string, notes string\").write.format(\"delta\").mode(\"append\").saveAsTable(INGESTION_AUDIT)\n",
    "\n",
    "# Final exit: always return structured JSON so Airflow can branch\n",
    "validated_bool = (len(errors) == 0)\n",
    "# TEST FAILURE\n",
    "#validated_bool = False\n",
    "result = {\"status\":\"VALIDATION_COMPLETE\", \"validated\": validated_bool, \"report\": report}\n",
    "dbutils.notebook.exit(json.dumps(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bcec05ed-7211-4ee8-8ae2-118c14a1351c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_validation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}