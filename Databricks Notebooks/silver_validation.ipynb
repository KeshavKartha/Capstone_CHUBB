{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4341bdea-f935-469f-a993-b4ae0a329bc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook: silver_validation\n",
    "# Path: /Workspace/Users/you/silver_validation\n",
    "# Widget: ingestion_batch_id (optional)\n",
    "#\n",
    "# Writes:\n",
    "#   - census.silver.validation_reports_v1  (append)\n",
    "#   - census.silver.ingestion_audit_v1     (append)\n",
    "#\n",
    "# Behavior:\n",
    "#   - If widget ingestion_batch_id provided -> validate that batch (fail early if not found)\n",
    "#   - If not provided -> auto-select most-recent batch with ingestion_attempts > 0\n",
    "#   - Always exits with structured JSON:\n",
    "#       {\"status\":\"VALIDATION_COMPLETE\", \"validated\": bool, \"report\": {...}}\n",
    "#\n",
    "from datetime import datetime\n",
    "import json\n",
    "from pyspark.sql import functions as F\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "FILE_REG_TABLE = \"census.bronze.file_registry_v1\"\n",
    "SILVER_PERSON = \"census.silver.dim_person\"\n",
    "SILVER_PERSON_HISTORY = \"census.silver.dim_person_history\"\n",
    "SILVER_LINEAGE = \"census.silver.lineage\"\n",
    "SILVER_HOUSEHOLD = \"census.silver.dim_household\"\n",
    "SILVER_REGION = \"census.silver.dim_region\"\n",
    "\n",
    "VALIDATION_TABLE = \"census.silver.validation_reports_v1\"\n",
    "INGESTION_AUDIT = \"census.silver.ingestion_audit_v1\"\n",
    "\n",
    "# thresholds (tunable)\n",
    "MAX_DUPLICATES_ALLOWED = 0\n",
    "PERSONS_TO_BRONZE_RATIO_WARN_THRESHOLD = 2.0\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def now():\n",
    "    return datetime.utcnow()\n",
    "\n",
    "def tidy_status_col(col):\n",
    "    return F.lower(F.trim(F.coalesce(F.col(col), F.lit(\"\"))))\n",
    "\n",
    "def available_batches_summary():\n",
    "    if not spark.catalog.tableExists(FILE_REG_TABLE):\n",
    "        return []\n",
    "    fr = spark.table(FILE_REG_TABLE)\n",
    "    s = fr.withColumn(\"status_norm\", tidy_status_col(\"ingestion_status\"))\n",
    "    agg = s.groupBy(\"ingestion_batch_id\").agg(\n",
    "        F.count(F.when(F.col(\"status_norm\") == \"pending\", True)).alias(\"pending_count\"),\n",
    "        F.count(F.when(F.col(\"status_norm\") == \"processing\", True)).alias(\"processing_count\"),\n",
    "        F.count(F.when(F.col(\"status_norm\") == \"succeeded\", True)).alias(\"succeeded_count\"),\n",
    "        F.count(F.when(F.col(\"status_norm\") == \"failed\", True)).alias(\"failed_count\"),\n",
    "        F.sum(F.coalesce(F.col(\"ingestion_attempts\"), F.lit(0))).alias(\"ingestion_attempts\"),\n",
    "        # --- FIX IS HERE: cast(\"string\") ---\n",
    "        F.max(\"updated_at\").cast(\"string\").alias(\"last_updated\"), \n",
    "        F.count(F.lit(1)).alias(\"total_files\")\n",
    "    )\n",
    "    return [r.asDict() for r in agg.orderBy(F.desc(\"ingestion_attempts\"), F.desc(\"succeeded_count\"), F.desc(\"last_updated\")).collect()]\n",
    "\n",
    "def pick_batch():\n",
    "    \"\"\"\n",
    "    Select the ingestion_batch_id to validate:\n",
    "      - prefer explicit widget\n",
    "      - otherwise choose the most recent batch with ingestion_attempts > 0 (prefers batches with succeeded files)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        widget_val = dbutils.widgets.get(\"ingestion_batch_id\")\n",
    "        if widget_val and widget_val.strip() != \"\":\n",
    "            return widget_val\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    if not spark.catalog.tableExists(FILE_REG_TABLE):\n",
    "        dbutils.notebook.exit(json.dumps({\"status\":\"VALIDATION_COMPLETE\",\"validated\": False, \"report\": {\"reason\":\"missing_file_registry_table\",\"table\": FILE_REG_TABLE}}))\n",
    "\n",
    "    fr = spark.table(FILE_REG_TABLE).withColumn(\"status_norm\", tidy_status_col(\"ingestion_status\"))\n",
    "    batches = fr.groupBy(\"ingestion_batch_id\").agg(\n",
    "        F.sum(F.coalesce(F.col(\"ingestion_attempts\"), F.lit(0))).alias(\"attempts\"),\n",
    "        F.sum(F.when(F.col(\"status_norm\") == \"succeeded\", 1).otherwise(0)).alias(\"succeeded_count\"),\n",
    "        F.max(\"updated_at\").alias(\"last_updated\")\n",
    "    ).filter(F.col(\"attempts\") > 0)\n",
    "\n",
    "    if batches.limit(1).count() == 0:\n",
    "        dbutils.notebook.exit(json.dumps({\"status\":\"VALIDATION_COMPLETE\",\"validated\": False, \"report\": {\"reason\":\"no_ingestion_attempts_found\"}}))\n",
    "\n",
    "    candidate = batches.orderBy(F.desc(\"succeeded_count\"), F.desc(\"last_updated\")).first()\n",
    "    return candidate[\"ingestion_batch_id\"]\n",
    "\n",
    "# ---------- start ----------\n",
    "ingestion_batch_id = pick_batch()\n",
    "start_ts = now()\n",
    "run_id = f\"silver_validation-{ingestion_batch_id}-{start_ts.strftime('%Y%m%dT%H%M%SZ')}\"\n",
    "\n",
    "errors = []\n",
    "warnings = []\n",
    "notes = {}\n",
    "\n",
    "# record available batches for debugging\n",
    "batches_summary = available_batches_summary()\n",
    "notes[\"available_batches_summary\"] = batches_summary\n",
    "\n",
    "# ---------- 0) dim_region existence and schema sanity ----------\n",
    "try:\n",
    "    if not spark.catalog.tableExists(SILVER_REGION):\n",
    "        errors.append(\"dim_region_missing\")\n",
    "    else:\n",
    "        region_df = spark.table(SILVER_REGION)\n",
    "        region_count = region_df.count()\n",
    "        if region_count == 0:\n",
    "            warnings.append(\"dim_region_empty\")\n",
    "        # basic expected columns check\n",
    "        expected_region_cols = {\"geoid\",\"region_name_standard\",\"iso_admin_code\"}\n",
    "        missing_region_cols = list(expected_region_cols - set(region_df.columns))\n",
    "        if missing_region_cols:\n",
    "            warnings.append(f\"dim_region_missing_columns:{missing_region_cols}\")\n",
    "except Exception as e:\n",
    "    errors.append(f\"dim_region_check_error:{str(e)}\")\n",
    "\n",
    "# ---------- 1) dim_person presence & minimal schema ---------- \n",
    "# should check if ANY processing happened for this batch, not just row counts\n",
    "person_count_for_report = 0\n",
    "processed_persons_count = 0  # Persons that were actually processed (new or updated)\n",
    "try:\n",
    "    if spark.catalog.tableExists(SILVER_PERSON):\n",
    "        # Count persons that have this batch_id (either newly inserted or updated)\n",
    "        persons_with_batch = spark.table(SILVER_PERSON).filter(\n",
    "            F.col(\"ingestion_batch_id\") == ingestion_batch_id\n",
    "        ).count()\n",
    "        \n",
    "        # Also check lineage table to see how many persons this batch contributed to\n",
    "        if spark.catalog.tableExists(SILVER_LINEAGE):\n",
    "            lineage_count = spark.table(SILVER_LINEAGE).filter(\n",
    "                F.col(\"ingestion_batch_id\") == ingestion_batch_id\n",
    "            ).count()\n",
    "            \n",
    "            # The actual count for reporting should be the maximum of these\n",
    "            # This accounts for both new inserts and updates\n",
    "            person_count_for_report = max(persons_with_batch, lineage_count)\n",
    "            \n",
    "            # Also count how many persons were actually processed (had their batch_id updated)\n",
    "            # This helps differentiate between \"no new data\" vs \"actual processing happened\"\n",
    "            processed_persons_count = persons_with_batch\n",
    "            \n",
    "            notes[\"person_count_with_batch\"] = persons_with_batch\n",
    "            notes[\"lineage_count_for_batch\"] = lineage_count\n",
    "            notes[\"processed_persons_count\"] = processed_persons_count\n",
    "            \n",
    "            # If lineage has records but dim_person doesn't, it might mean no changes\n",
    "            if lineage_count > 0 and persons_with_batch == 0:\n",
    "                notes[\"batch_processed_no_changes\"] = True\n",
    "                print(f\"NOTE: Batch {ingestion_batch_id} processed {lineage_count} persons but no attribute changes detected\")\n",
    "        \n",
    "        else:\n",
    "            person_count_for_report = persons_with_batch\n",
    "            notes[\"person_count_with_batch\"] = persons_with_batch\n",
    "        \n",
    "except Exception as e:\n",
    "    person_count_for_report = 0\n",
    "    warnings.append(f\"person_count_error:{str(e)}\")\n",
    "\n",
    "# ---------- 2) uniqueness: canonical_person_id + census_year + is_current unique ----------\n",
    "try:\n",
    "    if spark.catalog.tableExists(SILVER_PERSON):\n",
    "        # Check for ACTUAL duplicates in current records\n",
    "        dup_q = f\"\"\"\n",
    "            SELECT canonical_person_id, census_year, COUNT(*) as cnt\n",
    "            FROM {SILVER_PERSON}\n",
    "            WHERE is_current = true\n",
    "            GROUP BY canonical_person_id, census_year\n",
    "            HAVING COUNT(*) > 1\n",
    "        \"\"\"\n",
    "        dup_count = spark.sql(dup_q).count()\n",
    "        if dup_count > 0:\n",
    "            errors.append(f\"dup_current_records_count:{dup_count}\")\n",
    "            # Add debug info about sample duplicates\n",
    "            dup_samples = spark.sql(dup_q + \" LIMIT 5\").collect()\n",
    "            print(f\"DEBUG: Found {dup_count} duplicate groups. Samples: {dup_samples}\")\n",
    "except Exception as e:\n",
    "    errors.append(f\"dup_check_error:{str(e)}\")\n",
    "\n",
    "# ---------- 3) lineage completeness ----------\n",
    "try:\n",
    "    if spark.catalog.tableExists(SILVER_PERSON) and spark.catalog.tableExists(SILVER_LINEAGE):\n",
    "        # Check that for every lineage record in this batch, there's a corresponding person\n",
    "        # This is the main check - every person processed should be in lineage\n",
    "        missing_lineage_q = f\"\"\"\n",
    "            SELECT p.canonical_person_id, p.census_year\n",
    "            FROM {SILVER_PERSON} p\n",
    "            WHERE p.ingestion_batch_id = '{ingestion_batch_id}'\n",
    "              AND p.is_current = true\n",
    "              AND NOT EXISTS (\n",
    "                SELECT 1 \n",
    "                FROM {SILVER_LINEAGE} l\n",
    "                WHERE l.canonical_person_id = p.canonical_person_id\n",
    "                  AND l.census_year = p.census_year\n",
    "                  AND l.ingestion_batch_id = '{ingestion_batch_id}'\n",
    "              )\n",
    "        \"\"\"\n",
    "        missing_lineage = spark.sql(missing_lineage_q).count()\n",
    "        \n",
    "        if missing_lineage > 0:\n",
    "            errors.append(f\"persons_missing_lineage:{missing_lineage}\")\n",
    "            print(f\"ERROR: {missing_lineage} persons in dim_person with batch {ingestion_batch_id} but no lineage record\")\n",
    "            \n",
    "        # Also check the reverse - every lineage should have a current person (or historical)\n",
    "        # But note: lineage might exist for historical records too\n",
    "        missing_person_q = f\"\"\"\n",
    "            SELECT l.canonical_person_id, l.census_year\n",
    "            FROM {SILVER_LINEAGE} l\n",
    "            WHERE l.ingestion_batch_id = '{ingestion_batch_id}'\n",
    "              AND NOT EXISTS (\n",
    "                SELECT 1 \n",
    "                FROM {SILVER_PERSON} p\n",
    "                WHERE p.canonical_person_id = l.canonical_person_id\n",
    "                  AND p.census_year = l.census_year\n",
    "                  AND (p.is_current = true OR p.ingestion_batch_id = '{ingestion_batch_id}')\n",
    "              )\n",
    "        \"\"\"\n",
    "        missing_person = spark.sql(missing_person_q).count()\n",
    "        \n",
    "        if missing_person > 0:\n",
    "            warnings.append(f\"lineage_missing_persons:{missing_person}\")\n",
    "            print(f\"WARNING: {missing_person} lineage records without corresponding person\")\n",
    "            \n",
    "    else:\n",
    "        # lineage missing entirely -> warning\n",
    "        if not spark.catalog.tableExists(SILVER_LINEAGE):\n",
    "            warnings.append(\"lineage_table_missing\")\n",
    "except Exception as e:\n",
    "    errors.append(f\"lineage_check_error:{str(e)}\")\n",
    "\n",
    "# ---------- 4) household table sanity ----------\n",
    "try:\n",
    "    if not spark.catalog.tableExists(SILVER_HOUSEHOLD):\n",
    "        warnings.append(\"dim_household_missing\")\n",
    "    else:\n",
    "        hh_df = spark.table(SILVER_HOUSEHOLD).filter(F.col(\"ingestion_batch_id\") == ingestion_batch_id)\n",
    "        hh_count = hh_df.count()\n",
    "        notes[\"household_count\"] = hh_count\n",
    "        \n",
    "        # Note: Household count can be 0 even if batch processed persons\n",
    "        # This is because households are derived from person data, and might not change\n",
    "        \n",
    "        # check for households without ids\n",
    "        null_hh = hh_df.filter(F.col(\"household_id\").isNull()).count()\n",
    "        if null_hh > 0:\n",
    "            warnings.append(f\"household_null_count:{null_hh}\")\n",
    "except Exception as e:\n",
    "    warnings.append(f\"household_check_error:{str(e)}\")\n",
    "\n",
    "# ---------- 5) cross-check bronze->silver population sanity ----------\n",
    "bronze_count = 0\n",
    "try:\n",
    "    if spark.catalog.tableExists(\"census.bronze.individuals_raw_v1\"):\n",
    "        bronze_count = spark.table(\"census.bronze.individuals_raw_v1\").filter(\n",
    "            F.col(\"_ingestion_batch_id\") == ingestion_batch_id\n",
    "        ).count()\n",
    "        notes[\"bronze_count\"] = bronze_count\n",
    "except Exception:\n",
    "    bronze_count = 0\n",
    "\n",
    "# If processed_persons_count > 0, then at least some processing happened\n",
    "if bronze_count > 0 and processed_persons_count == 0:\n",
    "    # Check if this might be expected (same data re-ingested)\n",
    "    warnings.append(\"batch_processed_no_new_persons\")\n",
    "    print(f\"WARNING: Bronze has {bronze_count} rows but no new/updated persons in silver. This might be expected if data is identical.\")\n",
    "\n",
    "# The old warning about ratio is removed - it was misleading\n",
    "\n",
    "# ---------- 6) optional: basic attribute-level checks (age bounds, literacy, employment distributions) ----------\n",
    "try:\n",
    "    if spark.catalog.tableExists(SILVER_PERSON):\n",
    "        p = spark.table(SILVER_PERSON).filter(F.col(\"ingestion_batch_id\") == ingestion_batch_id)\n",
    "        if p.count() > 0:  \n",
    "            # age nulls fraction\n",
    "            if \"age\" in p.columns:\n",
    "                age_nulls = p.filter(F.col(\"age\").isNull()).count()\n",
    "                age_null_frac = age_nulls / max(1, p.count())\n",
    "                if age_null_frac > 0.05:\n",
    "                    warnings.append(f\"age_high_null_fraction:{age_null_frac:.4f}\")\n",
    "            # literacy present?\n",
    "            if \"literacy\" not in p.columns:\n",
    "                warnings.append(\"literacy_missing_in_person\")\n",
    "            # employment_status present?\n",
    "            if \"employment_status\" not in p.columns:\n",
    "                warnings.append(\"employment_status_missing_in_person\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ---------- 7) Check SCD2 history consistency ----------\n",
    "try:\n",
    "    if spark.catalog.tableExists(SILVER_PERSON_HISTORY):\n",
    "        history_count = spark.table(SILVER_PERSON_HISTORY).filter(\n",
    "            F.col(\"ingestion_batch_id\") == ingestion_batch_id\n",
    "        ).count()\n",
    "        notes[\"history_count_for_batch\"] = history_count\n",
    "    \n",
    "        if history_count > 0:\n",
    "            notes[\"scd2_triggered\"] = True\n",
    "            print(f\"INFO: SCD2 was triggered for {history_count} records (attribute changes detected)\")\n",
    "        else:\n",
    "            notes[\"scd2_triggered\"] = False\n",
    "            print(f\"INFO: No SCD2 triggered for batch {ingestion_batch_id} (no attribute changes)\")\n",
    "except Exception as e:\n",
    "    warnings.append(f\"history_check_error:{str(e)}\")\n",
    "\n",
    "# ---------- Assemble report ----------\n",
    "report = {\n",
    "    \"ingestion_batch_id\": ingestion_batch_id,\n",
    "    \"run_id\": run_id,\n",
    "    \"start_time\": start_ts.isoformat(),\n",
    "    \"end_time\": now().isoformat(),\n",
    "    \"errors\": errors,\n",
    "    \"warnings\": warnings,\n",
    "    \"notes\": notes,\n",
    "    \"bronze_count\": bronze_count,\n",
    "    \"person_count\": person_count_for_report,\n",
    "    \"processed_persons_count\": processed_persons_count,\n",
    "    \"scd2_triggered\": notes.get(\"scd2_triggered\", False),\n",
    "    \"batch_processed_no_changes\": notes.get(\"batch_processed_no_changes\", False)\n",
    "}\n",
    "\n",
    "# persist to validation_reports table (append)\n",
    "status_flag = \"PASS\" if not errors else \"ERROR\"\n",
    "try:\n",
    "    spark.createDataFrame([(ingestion_batch_id, run_id, now(), json.dumps(report), status_flag)],\n",
    "                          schema=\"ingestion_batch_id string, run_id string, report_time timestamp, report_json string, status string\") \\\n",
    "         .write.format(\"delta\").mode(\"append\").saveAsTable(VALIDATION_TABLE)\n",
    "except Exception as e:\n",
    "    # if writing the validation table fails, still exit with structured JSON indicating failure\n",
    "    report[\"validation_table_write_error\"] = str(e)\n",
    "\n",
    "# append ingestion audit row\n",
    "audit_notes = json.dumps({\"report_summary\": {\"errors\": len(errors), \"warnings\": len(warnings)}, \"note\": \"silver_validation run\"})\n",
    "audit_row = (ingestion_batch_id, run_id, start_ts, datetime.utcnow(), (\"SUCCEEDED\" if not errors else \"FAILED_VALIDATION\"), audit_notes)\n",
    "audit_schema = \"ingestion_batch_id string, run_id string, start_time timestamp, end_time timestamp, status string, notes string\"\n",
    "spark.createDataFrame([audit_row], schema=audit_schema).write.format(\"delta\").mode(\"append\").saveAsTable(INGESTION_AUDIT)\n",
    "\n",
    "# If hard errors exist, do NOT throw an exception â€” instead return structured result and let orchestrator decide\n",
    "validated_bool = (len(errors) == 0)\n",
    "result = {\"status\":\"VALIDATION_COMPLETE\", \"validated\": validated_bool, \"report\": report}\n",
    "\n",
    "print(f\"\\n=== VALIDATION SUMMARY ===\")\n",
    "print(f\"Batch ID: {ingestion_batch_id}\")\n",
    "print(f\"Errors: {len(errors)}\")\n",
    "print(f\"Warnings: {len(warnings)}\")\n",
    "print(f\"Bronze rows: {bronze_count}\")\n",
    "print(f\"Persons processed: {processed_persons_count}\")\n",
    "print(f\"SCD2 triggered: {notes.get('scd2_triggered', False)}\")\n",
    "print(f\"Validation result: {'PASS' if validated_bool else 'FAIL'}\")\n",
    "\n",
    "# final structured exit for Airflow to parse\n",
    "dbutils.notebook.exit(json.dumps(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0cb511e-8e25-4995-978d-059124eb0c09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_validation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}